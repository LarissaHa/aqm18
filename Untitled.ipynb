{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'neuralnet' was built under R version 3.4.4\"Warning message:\n",
      "\"package 'devtools' was built under R version 3.4.3\"\n",
      "Please cite as: \n",
      "\n",
      " Hlavac, Marek (2015). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n",
      " R package version 5.2. http://CRAN.R-project.org/package=stargazer \n",
      "\n",
      "Loading required package: bitops\n",
      "Warning message:\n",
      "\"package 'caret' was built under R version 3.4.4\"Loading required package: lattice\n",
      "Loading required package: ggplot2\n",
      "Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.4.4\"Warning message:\n",
      "\"package 'e1071' was built under R version 3.4.3\"Warning message:\n",
      "\"package 'h2o' was built under R version 3.4.4\"\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Your next step is to start H2O:\n",
      "    > h2o.init()\n",
      "\n",
      "For H2O package documentation, ask for help:\n",
      "    > ??h2o\n",
      "\n",
      "After starting H2O, you can use the Web UI at http://localhost:54321\n",
      "For more information visit http://docs.h2o.ai\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Attaching package: 'h2o'\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    cor, sd, var\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,\n",
      "    colnames<-, ifelse, is.character, is.factor, is.numeric, log,\n",
      "    log10, log1p, log2, round, signif, trunc\n",
      "\n",
      "Warning message:\n",
      "\"package 'statmod' was built under R version 3.4.4\"Warning message:\n",
      "\"package 'corrplot' was built under R version 3.4.4\"corrplot 0.84 loaded\n",
      "\n",
      "Attaching package: 'data.table'\n",
      "\n",
      "The following objects are masked from 'package:h2o':\n",
      "\n",
      "    hour, month, week, year\n",
      "\n",
      "Warning message:\n",
      "\"package 'ROSE' was built under R version 3.4.4\"Loaded ROSE 0.0-3\n",
      "\n",
      "Warning message:\n",
      "\"package 'reshape2' was built under R version 3.4.4\"\n",
      "Attaching package: 'reshape2'\n",
      "\n",
      "The following objects are masked from 'package:data.table':\n",
      "\n",
      "    dcast, melt\n",
      "\n",
      "Warning message:\n",
      "\"package 'ggpubr' was built under R version 3.4.4\"Loading required package: magrittr\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>97032</li>\n",
       "\t<li>25</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 97032\n",
       "\\item 25\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 97032\n",
       "2. 25\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 97032    25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "     \n",
       "y_hat    0    1\n",
       "    0 4108 2162\n",
       "    1 2166 1268"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "-7047.99667128909"
      ],
      "text/latex": [
       "-7047.99667128909"
      ],
      "text/markdown": [
       "-7047.99667128909"
      ],
      "text/plain": [
       "[1] -7047.997"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1.98500219554836</li>\n",
       "\t<li>-5.09023760384724</li>\n",
       "\t<li>-0.849381546234835</li>\n",
       "\t<li>-2.55898979413325</li>\n",
       "\t<li>-3.46513674274916</li>\n",
       "\t<li>-0.247511570512244</li>\n",
       "\t<li>1.50730752979539</li>\n",
       "\t<li>-2.88814603113879</li>\n",
       "\t<li>-0.0697631288396081</li>\n",
       "\t<li>-2.99861993490084</li>\n",
       "\t<li>2.54018520776017</li>\n",
       "\t<li>0.258431169315881</li>\n",
       "\t<li>-2.10298906200219</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1.98500219554836\n",
       "\\item -5.09023760384724\n",
       "\\item -0.849381546234835\n",
       "\\item -2.55898979413325\n",
       "\\item -3.46513674274916\n",
       "\\item -0.247511570512244\n",
       "\\item 1.50730752979539\n",
       "\\item -2.88814603113879\n",
       "\\item -0.0697631288396081\n",
       "\\item -2.99861993490084\n",
       "\\item 2.54018520776017\n",
       "\\item 0.258431169315881\n",
       "\\item -2.10298906200219\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1.98500219554836\n",
       "2. -5.09023760384724\n",
       "3. -0.849381546234835\n",
       "4. -2.55898979413325\n",
       "5. -3.46513674274916\n",
       "6. -0.247511570512244\n",
       "7. 1.50730752979539\n",
       "8. -2.88814603113879\n",
       "9. -0.0697631288396081\n",
       "10. -2.99861993490084\n",
       "11. 2.54018520776017\n",
       "12. 0.258431169315881\n",
       "13. -2.10298906200219\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1]  1.98500220 -5.09023760 -0.84938155 -2.55898979 -3.46513674 -0.24751157\n",
       " [7]  1.50730753 -2.88814603 -0.06976313 -2.99861993  2.54018521  0.25843117\n",
       "[13] -2.10298906"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "     \n",
       "y_hat    0    1\n",
       "    0 4137 2157\n",
       "    1 2137 1273"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.003561483238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "           \n",
       "result_test    0    1\n",
       "          0 4052 2203\n",
       "          1 2222 1227"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.372222222222222"
      ],
      "text/latex": [
       "0.372222222222222"
      ],
      "text/markdown": [
       "0.372222222222222"
      ],
      "text/plain": [
       "[1] 0.3722222222"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.369463869463869"
      ],
      "text/latex": [
       "0.369463869463869"
      ],
      "text/markdown": [
       "0.369463869463869"
      ],
      "text/plain": [
       "[1] 0.3694638695"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.356737897950283"
      ],
      "text/latex": [
       "0.356737897950283"
      ],
      "text/markdown": [
       "0.356737897950283"
      ],
      "text/plain": [
       "[1] 0.356737898"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.521416738976098"
      ],
      "text/latex": [
       "0.521416738976098"
      ],
      "text/markdown": [
       "0.521416738976098"
      ],
      "text/plain": [
       "[1] 0.521416739"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.486145574855252"
      ],
      "text/latex": [
       "0.486145574855252"
      ],
      "text/markdown": [
       "0.486145574855252"
      ],
      "text/plain": [
       "[1] 0.4861455749"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.277461041429114"
      ],
      "text/latex": [
       "0.277461041429114"
      ],
      "text/markdown": [
       "0.277461041429114"
      ],
      "text/plain": [
       "[1] 0.2774610414"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "     \n",
       "y_hat    0    1\n",
       "    0 4400 2049\n",
       "    1 1874 1381"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.595733718054411"
      ],
      "text/latex": [
       "0.595733718054411"
      ],
      "text/markdown": [
       "0.595733718054411"
      ],
      "text/plain": [
       "[1] 0.5957337181"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.413163799551234"
      ],
      "text/latex": [
       "0.413163799551234"
      ],
      "text/markdown": [
       "0.413163799551234"
      ],
      "text/plain": [
       "[1] 0.4131637996"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>6</th><td>0</td></tr>\n",
       "\t<tr><th scope=row>11</th><td>0</td></tr>\n",
       "\t<tr><th scope=row>31</th><td>0</td></tr>\n",
       "\t<tr><th scope=row>32</th><td>0</td></tr>\n",
       "\t<tr><th scope=row>37</th><td>0</td></tr>\n",
       "\t<tr><th scope=row>38</th><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "\t6 & 0\\\\\n",
       "\t11 & 0\\\\\n",
       "\t31 & 0\\\\\n",
       "\t32 & 0\\\\\n",
       "\t37 & 0\\\\\n",
       "\t38 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 6 | 0 | \n",
       "| 11 | 0 | \n",
       "| 31 | 0 | \n",
       "| 32 | 0 | \n",
       "| 37 | 0 | \n",
       "| 38 | 0 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   [,1]\n",
       "6  0   \n",
       "11 0   \n",
       "31 0   \n",
       "32 0   \n",
       "37 0   \n",
       "38 0   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.681265457543281"
      ],
      "text/latex": [
       "0.681265457543281"
      ],
      "text/markdown": [
       "0.681265457543281"
      ],
      "text/plain": [
       "[1] 0.6812654575"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.571301544959431"
      ],
      "text/latex": [
       "0.571301544959431"
      ],
      "text/markdown": [
       "0.571301544959431"
      ],
      "text/plain": [
       "[1] 0.571301545"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(neuralnet)\n",
    "library(devtools)\n",
    "library(stargazer)\n",
    "library(RCurl)\n",
    "library(jsonlite)\n",
    "library(caret)\n",
    "library(ggplot2)\n",
    "library(e1071)\n",
    "library(h2o)\n",
    "library(statmod)\n",
    "library(MASS)\n",
    "library(corrplot)\n",
    "library(data.table)\n",
    "library(ROSE)\n",
    "library(reshape2)\n",
    "library(ggpubr)\n",
    "\n",
    "F1 <- function(table){\n",
    "    tp <- table[2,2]\n",
    "    tn <- table[1,1]\n",
    "    fp <- table[2,1]\n",
    "    fn <- table[1,2]\n",
    "    precision <- tp / (tp + fp)\n",
    "    recall <- tp / (tp + fn)\n",
    "    result <- (2 * precision * recall) / (precision + recall)\n",
    "    return(result)\n",
    "}\n",
    "\n",
    "ACC <- function(table){\n",
    "    tp <- table[2,2]\n",
    "    tn <- table[1,1]\n",
    "    fp <- table[2,1]\n",
    "    fn <- table[1,2]\n",
    "    acc <- (tn + tp) / (tn + tp + fn + fp)\n",
    "    return(acc)\n",
    "}\n",
    "\n",
    "ess1 <- read.table(\"C:/Users/laris/Desktop/MMDS/Semester3-FSS2018/Advanced-Quantitative-Methods/paper/ess1_with-vars-nolabel.csv\", header=TRUE, sep=\",\") \n",
    "\n",
    "ess6 <- read.table(\"C:/Users/laris/Desktop/MMDS/Semester3-FSS2018/Advanced-Quantitative-Methods/paper/ess6_with-vars-nolabel.csv\", header=TRUE, sep=\",\") \n",
    "\n",
    "ess1$round1 = 1\n",
    "ess6$round1 = 0\n",
    "\n",
    "data <- rbind(ess1, ess6)\n",
    "\n",
    "dim(data)\n",
    "\n",
    "data.complete <- data\n",
    "\n",
    "for(i in c(4,5,6,10,11,12,13,14,15,16,17,18,19,20,21,22,23)){\n",
    "    data.complete[is.na(data.complete[,i]), i] <- mean(data.complete[,i], na.rm = TRUE)\n",
    "}\n",
    "\n",
    "data.complete[is.na(data.complete[,3]), 3] <- 1\n",
    "data.complete[is.na(data.complete[,7]), 7] <- 1\n",
    "data.complete[is.na(data.complete[,8]), 8] <- 0\n",
    "data.complete[is.na(data.complete[,9]), 9] <- 0\n",
    "\n",
    "data.scaled <- cbind(data.complete[c(1, 3, 7, 8, 9, 25)], scale(data.complete[-c(1, 2, 3, 7, 8, 9, 24, 25)]))\n",
    "\n",
    "#head(data.scaled)\n",
    "\n",
    "#write.csv(data.complete, file = \"ess-complete.csv\")\n",
    "\n",
    "#write.csv(data.scaled, file = \"ess-scaled.csv\")\n",
    "\n",
    "#data.scaled <- read.table(\"ess-scaled.csv\", header=TRUE, sep=\",\")\n",
    "#data.scaled <- data.scaled[,-1]\n",
    "\n",
    "#head(data.scaled)\n",
    "\n",
    "smp_size <- floor(0.9 * nrow(data.scaled))\n",
    "set.seed(123)\n",
    "train_ind <- sample(seq_len(nrow(data.scaled)), size = smp_size)\n",
    "train <- data.scaled[train_ind,]\n",
    "test <- data.scaled[-train_ind,]\n",
    "\n",
    "train.bal <- ovun.sample(volact ~ ., data = train, method = \"over\", N = 114264)$data\n",
    "\n",
    "col <- test$volact\n",
    "col[test$volact == 1] <- adjustcolor(\"orange\", alpha = 0.3)\n",
    "col[test$volact == 0] <- adjustcolor(\"blue\", alpha = 0.3)\n",
    "\n",
    "#plot(test$wkhtot, test$social_trust, col = col, pch = 19,\n",
    "#     main = \"True Relationship \\n (scaled values)\",\n",
    "#     bty = \"n\", las = 1,\n",
    "#     xlab = \"Total Work Hours (per Week)\", ylab = \"Social Trust\")\n",
    "\n",
    "ll_logit <- function(theta, y, X) {\n",
    "    \n",
    "  # theta consists merely of beta (dim is ncol(X))\n",
    "    beta <- theta[1:ncol(X)]\n",
    "    # linear predictor; make sure that X is stored as.matrix\n",
    "    mu <- X %*% beta\n",
    "    # response function\n",
    "    p <- 1/(1 + exp(-mu))\n",
    "    # log-likelihood\n",
    "    ll <- y * log(p) + (1 - y) * log(1 - p)\n",
    "    # sum\n",
    "    ll <- sum(ll)\n",
    "    return(ll)\n",
    "}\n",
    "\n",
    "y <- test$volact\n",
    "X <- cbind(1, test$wkhtot, test$social_trust)\n",
    "\n",
    "startvals <- c(0, 0, 0)\n",
    "\n",
    "res <- optim(par = startvals,fn = ll_logit, y = y, X = X,\n",
    "              control = list(fnscale = -1),\n",
    "              method = \"BFGS\"\n",
    "              ) \n",
    "\n",
    "mu <- X %*% res$par\n",
    "\n",
    "p <- 1/(1 + exp(-mu))\n",
    "\n",
    "y_hat <- rbinom(nrow(p), 1, p)\n",
    "col <- y_hat\n",
    "col[y_hat == 1] <- adjustcolor(\"orange\", alpha = 0.3)\n",
    "col[y_hat == 0] <- adjustcolor(\"blue\", alpha = 0.3)\n",
    "\n",
    "#plot(test$wkhtot, test$social_trust, col = col, pch = 19,\n",
    "#      main = \"True Relationship\",\n",
    "#     bty = \"n\", las = 1,\n",
    "#     xlab = \"x1\", ylab = \"x2\")\n",
    "\n",
    "logit_pred <- table(y_hat, test$volact)\n",
    "logit_pred\n",
    "\n",
    "ll_simple_nn <- function(theta, y, X){\n",
    "  \n",
    "  gamma <- theta[1:4]\n",
    "  beta_neuron1 <- theta[5:7]\n",
    "  beta_neuron2 <- theta[8:10]\n",
    "  beta_neuron3 <- theta[11:13]\n",
    "  \n",
    "  mu_neuron1 <- X %*% beta_neuron1\n",
    "  mu_neuron2 <- X %*% beta_neuron2\n",
    "  mu_neuron3 <- X %*% beta_neuron3\n",
    "  \n",
    "  logitResponse <- function(mu) 1 / (1+exp(-mu))\n",
    "  \n",
    "  p_neuron1 <- logitResponse(mu_neuron1)\n",
    "  p_neuron2 <- logitResponse(mu_neuron2)\n",
    "  p_neuron3 <- logitResponse(mu_neuron3)\n",
    "  \n",
    "  Z <- cbind(1, p_neuron1, p_neuron2, p_neuron3)\n",
    "  \n",
    "  mu <- Z %*% gamma\n",
    "  \n",
    "  p <- logitResponse(mu)\n",
    "  \n",
    "  ll <- y * log(p) + (1 - y) * log(1 - p)\n",
    "  \n",
    "  ll <- sum(ll)\n",
    "  return(ll)\n",
    "}\n",
    "\n",
    "# initial values\n",
    "startvals <- rnorm(13)\n",
    "\n",
    "ll_simple_nn(startvals, y, X)\n",
    "\n",
    "# optimize\n",
    "resNN <- optim(par = startvals, fn = ll_simple_nn, y = y, X = X,\n",
    "              control = list(fnscale = -1),\n",
    "              hessian = F,\n",
    "              method = \"BFGS\"\n",
    "              ) \n",
    "\n",
    "resNN$par\n",
    "\n",
    "gammaEst <- resNN$par[1:4]\n",
    "beta_neuron1Est <- resNN$par[5:7]\n",
    "beta_neuron2Est <- resNN$par[8:10]\n",
    "beta_neuron3Est <- resNN$par[11:13]\n",
    "\n",
    "mu_neuron1Est <- X %*% beta_neuron1Est\n",
    "mu_neuron2Est <- X %*% beta_neuron2Est\n",
    "mu_neuron3Est <- X %*% beta_neuron3Est\n",
    "\n",
    "logitResponse <- function(mu) 1/(1+exp(-mu))\n",
    "\n",
    "p_neuron1Est <- logitResponse(mu_neuron1Est)\n",
    "p_neuron2Est <- logitResponse(mu_neuron2Est)\n",
    "p_neuron3Est <- logitResponse(mu_neuron3Est)\n",
    "\n",
    "Z <- cbind(1, p_neuron1Est, p_neuron2Est, p_neuron3Est )\n",
    "\n",
    "mu <- Z %*% gammaEst\n",
    "\n",
    "p <- logitResponse(mu)\n",
    "\n",
    "\n",
    "y_hat <- rbinom(nrow(p),1,p)\n",
    "col <- y_hat\n",
    "\n",
    "col[y_hat == 1] <- adjustcolor(\"orange\", alpha = 0.3)\n",
    "col[y_hat == 0] <- adjustcolor(\"blue\", alpha = 0.3)\n",
    "\n",
    "#plot(X[, 2], X[, 3], col = col, pch = 19,\n",
    "#     main = \"Predicted Values from a Neural Net\",\n",
    "#     bty = \"n\", las = 1,\n",
    "#     xlab = \"x1\", ylab = \"x2\")\n",
    "\n",
    "nn_pred <- table(y_hat, test$volact)\n",
    "nn_pred\n",
    "\n",
    "m <- neuralnet(volact ~ wkhtot + social_trust, \n",
    "              train, hidden = 1)\n",
    "\n",
    "p <- compute(m, test[,c(11, 14)])\n",
    "\n",
    "predictions <- p$net.result\n",
    "\n",
    "result_test <- rbinom(nrow(predictions),1,predictions)\n",
    "result3 <- ifelse(predictions >= 0.3, 1, 0)\n",
    "result2 <- ifelse(predictions >= 0.2, 1, 0)\n",
    "result4 <- ifelse(predictions >= 0.4, 1, 0)\n",
    "\n",
    "print(cor(result_test, test$volact))\n",
    "\n",
    "prenn_pred <- table(result_test, test$volact)\n",
    "prenn_pred\n",
    "\n",
    "#png(\"basic_nn.png\", width = 800, height = 600)\n",
    "#plot.nnet(m)\n",
    "#dev.off()\n",
    "\n",
    "F1(nn_pred)\n",
    "F1(logit_pred)\n",
    "F1(prenn_pred)\n",
    "F1(table(result2, test$volact))\n",
    "F1(table(result3, test$volact))\n",
    "F1(table(result4, test$volact))\n",
    "\n",
    "y <- train$volact\n",
    "X <- as.matrix(cbind(1, train[,-1]))\n",
    "\n",
    "startvals <- c(rep(0, 23))\n",
    "res <- optim(par = startvals,fn = ll_logit, y = y, X = X,\n",
    "              control = list(fnscale = -1),\n",
    "              method = \"BFGS\"\n",
    "              ) \n",
    "\n",
    "mu <- as.matrix(cbind(1, test[,-1])) %*% res$par\n",
    "p <- 1/(1 + exp(-mu))\n",
    "y_hat <- rbinom(nrow(p), 1, p)\n",
    "\n",
    "table(y_hat, test$volact)\n",
    "ACC(table(y_hat, test$volact))\n",
    "F1(table(y_hat, test$volact))\n",
    "\n",
    "y_hat1 <- ifelse(p > 0.1, 1, 0)\n",
    "y_hat2 <- ifelse(p > 0.2, 1, 0)\n",
    "y_hat3 <- ifelse(p > 0.3, 1, 0)\n",
    "y_hat4 <- ifelse(p > 0.4, 1, 0)\n",
    "y_hat5 <- ifelse(p > 0.5, 1, 0)\n",
    "y_hat6 <- ifelse(p > 0.6, 1, 0)\n",
    "y_hat7 <- ifelse(p > 0.7, 1, 0)\n",
    "y_hat8 <- ifelse(p > 0.8, 1, 0)\n",
    "y_hat9 <- ifelse(p > 0.9, 1, 0)\n",
    "\n",
    "head(y_hat9)\n",
    "\n",
    "threshold <- cbind(seq(0.1, 0.8, length.out = 8),\n",
    "    \n",
    "c(ACC(table(y_hat1, test$volact)),\n",
    "ACC(table(y_hat2, test$volact)),\n",
    "ACC(table(y_hat3, test$volact)),\n",
    "ACC(table(y_hat4, test$volact)),\n",
    "ACC(table(y_hat5, test$volact)),\n",
    "ACC(table(y_hat6, test$volact)),\n",
    "ACC(table(y_hat7, test$volact)),\n",
    "ACC(table(y_hat8, test$volact))),\n",
    "#ACC(table(y_hat9, test$volact))),\n",
    "\n",
    "c(F1(table(y_hat1, test$volact)),\n",
    "F1(table(y_hat2, test$volact)),\n",
    "F1(table(y_hat3, test$volact)),\n",
    "F1(table(y_hat4, test$volact)),\n",
    "F1(table(y_hat5, test$volact)),\n",
    "F1(table(y_hat6, test$volact)),\n",
    "F1(table(y_hat7, test$volact)),\n",
    "F1(table(y_hat8, test$volact))))\n",
    "#F1(table(y_hat9, test$volact))))\n",
    "\n",
    "max(threshold[,2])\n",
    "max(threshold[,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         \n",
       "y_hat.bal    0    1\n",
       "        0 3506 1543\n",
       "        1 2768 1887"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.555750206100577"
      ],
      "text/latex": [
       "0.555750206100577"
      ],
      "text/markdown": [
       "0.555750206100577"
      ],
      "text/plain": [
       "[1] 0.5557502061"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.466790352504638"
      ],
      "text/latex": [
       "0.466790352504638"
      ],
      "text/markdown": [
       "0.466790352504638"
      ],
      "text/plain": [
       "[1] 0.4667903525"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.67858615004122"
      ],
      "text/latex": [
       "0.67858615004122"
      ],
      "text/markdown": [
       "0.67858615004122"
      ],
      "text/plain": [
       "[1] 0.67858615"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.576141621404144"
      ],
      "text/latex": [
       "0.576141621404144"
      ],
      "text/markdown": [
       "0.576141621404144"
      ],
      "text/plain": [
       "[1] 0.5761416214"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc <- ggplot(data.frame(threshold),aes(threshold[,1],threshold[,2]))+geom_line(aes(color=\"Accuracy\"))+\n",
    "    labs(color=\" \") + \n",
    "    ylab(\"accuracy value\") + xlab(\"threshold\")\n",
    "f1 <- ggplot(data.frame(threshold),aes(threshold[,1],threshold[,4]))+geom_line(aes(color=\"F1\"))+\n",
    "    labs(color=\" \") + \n",
    "    ylab(\"f1 value\") + xlab(\"threshold\")\n",
    "#png(\"evaluation_log_threshold_unbal.png\") \n",
    "#myplot <- ggarrange(acc, f1 + rremove(\"x.text\"), \n",
    "#          labels = c(\"ACC\", \"F1\"),\n",
    "#          ncol = 2, nrow = 1)\n",
    "#print(myplot)\n",
    "#dev.off()\n",
    "#print(myplot)\n",
    "\n",
    "y.bal <- train.bal$volact\n",
    "X.bal <- as.matrix(cbind(1, train.bal[,-1]))\n",
    "\n",
    "startvals <- c(rep(0, 23))\n",
    "res <- optim(par = startvals,fn = ll_logit, y = y.bal, X = X.bal,\n",
    "              control = list(fnscale = -1), hessian = TRUE,\n",
    "              method = \"BFGS\"\n",
    "              ) \n",
    "\n",
    "mu <- as.matrix(cbind(1, test[,-1])) %*% res$par\n",
    "\n",
    "p <- 1/(1 + exp(-mu))\n",
    "\n",
    "y_hat.bal <- rbinom(nrow(p), 1, p)\n",
    "\n",
    "table(y_hat.bal, test$volact)\n",
    "ACC(table(y_hat.bal, test$volact))\n",
    "F1(table(y_hat.bal, test$volact))\n",
    "\n",
    "y_hat1 <- ifelse(p > 0.1, 1, 0)\n",
    "y_hat2 <- ifelse(p > 0.2, 1, 0)\n",
    "y_hat3 <- ifelse(p > 0.3, 1, 0)\n",
    "y_hat4 <- ifelse(p > 0.4, 1, 0)\n",
    "y_hat5 <- ifelse(p > 0.5, 1, 0)\n",
    "y_hat6 <- ifelse(p > 0.6, 1, 0)\n",
    "y_hat7 <- ifelse(p > 0.7, 1, 0)\n",
    "y_hat8 <- ifelse(p > 0.8, 1, 0)\n",
    "y_hat9 <- ifelse(p > 0.9, 1, 0)\n",
    "\n",
    "threshold <- cbind(seq(0.1, 0.9, length.out = 9),\n",
    "    \n",
    "c(ACC(table(y_hat1, test$volact)),\n",
    "ACC(table(y_hat2, test$volact)),\n",
    "ACC(table(y_hat3, test$volact)),\n",
    "ACC(table(y_hat4, test$volact)),\n",
    "ACC(table(y_hat5, test$volact)),\n",
    "ACC(table(y_hat6, test$volact)),\n",
    "ACC(table(y_hat7, test$volact)),\n",
    "ACC(table(y_hat8, test$volact)),\n",
    "ACC(table(y_hat9, test$volact))),\n",
    "\n",
    "c(F1(table(y_hat1, test$volact)),\n",
    "F1(table(y_hat2, test$volact)),\n",
    "F1(table(y_hat3, test$volact)),\n",
    "F1(table(y_hat4, test$volact)),\n",
    "F1(table(y_hat5, test$volact)),\n",
    "F1(table(y_hat6, test$volact)),\n",
    "F1(table(y_hat7, test$volact)),\n",
    "F1(table(y_hat8, test$volact)),\n",
    "F1(table(y_hat9, test$volact))))\n",
    "\n",
    "max(threshold[,2])\n",
    "max(threshold[,3])\n",
    "\n",
    "acc <- ggplot(data.frame(threshold),aes(threshold[,1],threshold[,2]))+geom_line(aes(color=\"Accuracy\"))+\n",
    "    labs(color=\" \") + \n",
    "    ylab(\"accuracy value\") + xlab(\"threshold\")\n",
    "f1 <- ggplot(data.frame(threshold),aes(threshold[,1],threshold[,4]))+geom_line(aes(color=\"F1\"))+\n",
    "    labs(color=\" \") + \n",
    "    ylab(\"f1 value\") + xlab(\"threshold\")\n",
    "#png(\"evaluation_log_threshold_bal.png\") \n",
    "#myplot <- ggarrange(acc, f1 + rremove(\"x.text\"), \n",
    "#          labels = c(\"ACC\", \"F1\"),\n",
    "#          ncol = 2, nrow = 1)\n",
    "#print(myplot)\n",
    "#dev.off()\n",
    "#print(myplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model <- glm(y ~ X, family=binomial(link='logit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary(ols_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "range <- seq(-4, 2, length.out = 20)\n",
    "sel <- 7\n",
    "plot.data <- matrix(NA, nrow = length(range), ncol = ncol(test[,-1]))\n",
    "for(i in 1:ncol(test[,-1])){\n",
    "    plot.data[,i] <- mean(test[,i])\n",
    "}\n",
    "for(i in 1:length(range)){\n",
    "    plot.data[i, sel] <- range[i]\n",
    "}\n",
    "\n",
    "mu <- as.matrix(cbind(1, plot.data)) %*% res$par\n",
    "p <- 1/(1 + exp(-mu))\n",
    "\n",
    "age.volact <- data.frame(cbind(range, p))\n",
    "\n",
    "#m <- neuralnet(volact ~ sclmeet, \n",
    "#              train, hidden = 1)\n",
    "\n",
    "#p <- compute(m, test[,13])\n",
    "\n",
    "#predictions <- p$net.result\n",
    "\n",
    "#print(cor(predictions, test$volact))\n",
    "\n",
    "#result <- ifelse(predictions >= 0.3, 1, 0)\n",
    "#ACC(table(result, test$volact))\n",
    "#F1(table(result, test$volact))\n",
    "\n",
    "#plot.nnet(m)\n",
    "\n",
    "#m2 <- neuralnet(volact ~ sclmeet + social_trust + tolerance + self_realisation + solidarity, \n",
    "#              train, hidden = 2)\n",
    "\n",
    "#p2 <- compute(m2, test[,c(13, 14, 15, 16, 17)])\n",
    "\n",
    "#predictions2 <- p2$net.result\n",
    "\n",
    "#print(cor(predictions2, test$volact))\n",
    "\n",
    "#result3 <- ifelse(predictions2 >= 0.3, 1, 0)\n",
    "#result2 <- ifelse(predictions2 >= 0.2, 1, 0)\n",
    "#result4 <- ifelse(predictions2 >= 0.4, 1, 0)\n",
    "\n",
    "#ACC(table(result4, test$volact))\n",
    "#F1(table(result4, test$volact))\n",
    "\n",
    "#png(\"more-complex_nn.png\", width = 800, height = 600)\n",
    "#plot.nnet(m2)\n",
    "#dev.off()\n",
    "\n",
    "#m3 <- neuralnet(volact ~ round1 + female + yrbrn + eduyrs + domicil + married + \n",
    "#               children + houseperson + wkhtot + church + sclmeet + social_trust + \n",
    "#               tolerance + self_realisation + solidarity + tvpol + tvtot + \n",
    "#               political_interest + trust_exe + trust_leg + trstep + stfdem, \n",
    "#               train, hidden = 1)\n",
    "\n",
    "#p3 <- compute(m3, test[,-c(1, 2, 25)])\n",
    "\n",
    "#predictions3 <- p3$net.result\n",
    "\n",
    "#print(cor(predictions3, test$volact))\n",
    "\n",
    "#library(nnet)\n",
    "#png(\"total-input_nn.png\", width = 800, height = 600)\n",
    "#plot.nnet(m3)\n",
    "#dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.X <- train[, -1]\n",
    "train.y <- train[, 1]\n",
    "train.y <- factor(train.y, levels = 0:1)\n",
    "train.X.bal <- train[, -1]\n",
    "train.y.bal <- train[, 1]\n",
    "train.y.bal <- factor(train.y, levels = 0:1)\n",
    "test.X <- test[, -1]\n",
    "test.y <- test[, 1]\n",
    "test.y <- factor(test.y, levels = 0:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aus: R deep learning essentials\n",
    "#m20.0 <- train(x = train.X, y = train.y,\n",
    "#           method = \"nnet\", \n",
    "#           tuneGrid = expand.grid(\n",
    "#           .size = c(20),\n",
    "#           .decay = 0),\n",
    "#           trControl= trainControl(method = \"none\"),\n",
    "#           MaxNWts = 1000,\n",
    "#           maxit = 100)\n",
    "#m20.1 <- train(x = train.X, y = train.y,\n",
    "#           method = \"nnet\", \n",
    "#           tuneGrid = expand.grid(\n",
    "#           .size = c(20),\n",
    "#           .decay = 0.1),\n",
    "#           trControl= trainControl(method = \"none\"),\n",
    "#           MaxNWts = 1000,\n",
    "#           maxit = 100)\n",
    "#m20.2 <- train(x = train.X, y = train.y,\n",
    "#           method = \"nnet\", \n",
    "#           tuneGrid = expand.grid(\n",
    "#           .size = c(20),\n",
    "#           .decay = 0.2),\n",
    "#           trControl= trainControl(method = \"none\"),\n",
    "#           MaxNWts = 1000,\n",
    "#           maxit = 100)\n",
    "#m20.3 <- train(x = train.X, y = train.y,\n",
    "#           method = \"nnet\", \n",
    "#           tuneGrid = expand.grid(\n",
    "#           .size = c(20),\n",
    "#           .decay = 0.3),\n",
    "#           trControl= trainControl(method = \"none\"),\n",
    "#           MaxNWts = 1000,\n",
    "#           maxit = 100)\n",
    "#m20.4 <- train(x = train.X, y = train.y,\n",
    "#           method = \"nnet\", \n",
    "#           tuneGrid = expand.grid(\n",
    "#           .size = c(20),\n",
    "#           .decay = 0.4),\n",
    "#           trControl= trainControl(method = \"none\"),\n",
    "#           MaxNWts = 1000,\n",
    "#           maxit = 100)\n",
    "#m20.5 <- train(x = train.X, y = train.y,\n",
    "#           method = \"nnet\", \n",
    "#           tuneGrid = expand.grid(\n",
    "#           .size = c(20),\n",
    "#           .decay = 0.5),\n",
    "#           trControl= trainControl(method = \"none\"),\n",
    "#           MaxNWts = 1000,\n",
    "#           maxit = 100)\n",
    "#m20.6 <- train(x = train.X, y = train.y,\n",
    "#           method = \"nnet\", \n",
    "#           tuneGrid = expand.grid(\n",
    "#           .size = c(20),\n",
    "#           .decay = 0.6),\n",
    "#           trControl= trainControl(method = \"none\"),\n",
    "#           MaxNWts = 1000,\n",
    "#           maxit = 100)\n",
    "#m20.7 <- train(x = train.X, y = train.y,\n",
    "#           method = \"nnet\", \n",
    "#           tuneGrid = expand.grid(\n",
    "#           .size = c(20),\n",
    "#           .decay = 0.7),\n",
    "#           trControl= trainControl(method = \"none\"),\n",
    "#           MaxNWts = 1000,\n",
    "#           maxit = 100)\n",
    "#m20.8 <- train(x = train.X, y = train.y,\n",
    "#           method = \"nnet\", \n",
    "#           tuneGrid = expand.grid(\n",
    "#           .size = c(20),\n",
    "#           .decay = 0.8),\n",
    "#           trControl= trainControl(method = \"none\"),\n",
    "#           MaxNWts = 1000,\n",
    "#           maxit = 100)\n",
    "#m20.9 <- train(x = train.X, y = train.y,\n",
    "#           method = \"nnet\", \n",
    "#           tuneGrid = expand.grid(\n",
    "#           .size = c(20),\n",
    "#           .decay = 0.9),\n",
    "#           trControl= trainControl(method = \"none\"),\n",
    "#           MaxNWts = 1000,\n",
    "#           maxit = 100)\n",
    "#m20.10 <- train(x = train.X, y = train.y,\n",
    "#           method = \"nnet\", \n",
    "#           tuneGrid = expand.grid(\n",
    "#           .size = c(20),\n",
    "#           .decay = 1),\n",
    "#           trControl= trainControl(method = \"none\"),\n",
    "#           MaxNWts = 1000,\n",
    "#           maxit = 100)\n",
    "\n",
    "#yhat20.0 <- predict(m20.0)\n",
    "#yhat20.1 <- predict(m20.1)\n",
    "#yhat20.2 <- predict(m20.2)\n",
    "#yhat20.3 <- predict(m20.3)\n",
    "#yhat20.4 <- predict(m20.4)\n",
    "#yhat20.5 <- predict(m20.5)\n",
    "#yhat20.6 <- predict(m20.6)\n",
    "#yhat20.7 <- predict(m20.7)\n",
    "#yhat20.8 <- predict(m20.8)\n",
    "#yhat20.9 <- predict(m20.9)\n",
    "#yhat20.10 <- predict(m20.10)\n",
    "#yhat_unseen20.0 <- predict(m20.0, as.matrix(test.X))\n",
    "#yhat_unseen20.1 <- predict(m20.1, as.matrix(test.X))\n",
    "#yhat_unseen20.2 <- predict(m20.2, as.matrix(test.X))\n",
    "#yhat_unseen20.3 <- predict(m20.3, as.matrix(test.X))\n",
    "#yhat_unseen20.4 <- predict(m20.4, as.matrix(test.X))\n",
    "#yhat_unseen20.5 <- predict(m20.5, as.matrix(test.X))\n",
    "#yhat_unseen20.6 <- predict(m20.6, as.matrix(test.X))\n",
    "#yhat_unseen20.7 <- predict(m20.7, as.matrix(test.X))\n",
    "#yhat_unseen20.8 <- predict(m20.8, as.matrix(test.X))\n",
    "#yhat_unseen20.9 <- predict(m20.9, as.matrix(test.X))\n",
    "#yhat_unseen20.10 <- predict(m20.10, as.matrix(test.X))\n",
    "\n",
    "#measures <- c(\"AccuracyNull\", \"Accuracy\", \"AccuracyLower\", \"AccuracyUpper\")\n",
    "\n",
    "#n20.0.insample <- caret::confusionMatrix(xtabs(~train.y + yhat20.0))\n",
    "#n20.1.insample <- caret::confusionMatrix(xtabs(~train.y + yhat20.1))\n",
    "#n20.2.insample <- caret::confusionMatrix(xtabs(~train.y + yhat20.2))\n",
    "#n20.3.insample <- caret::confusionMatrix(xtabs(~train.y + yhat20.3))\n",
    "#n20.4.insample <- caret::confusionMatrix(xtabs(~train.y + yhat20.4))\n",
    "#n20.5.insample <- caret::confusionMatrix(xtabs(~train.y + yhat20.5))\n",
    "#n20.6.insample <- caret::confusionMatrix(xtabs(~train.y + yhat20.6))\n",
    "#n20.7.insample <- caret::confusionMatrix(xtabs(~train.y + yhat20.7))\n",
    "#n20.8.insample <- caret::confusionMatrix(xtabs(~train.y + yhat20.8))\n",
    "#n20.9.insample <- caret::confusionMatrix(xtabs(~train.y + yhat20.9))\n",
    "#n20.10.insample <- caret::confusionMatrix(xtabs(~train.y + yhat20.10))\n",
    "#n20.0.outsample <- caret::confusionMatrix(xtabs(~test.y + yhat_unseen20.0))\n",
    "#n20.1.outsample <- caret::confusionMatrix(xtabs(~test.y + yhat_unseen20.1))\n",
    "#n20.2.outsample <- caret::confusionMatrix(xtabs(~test.y + yhat_unseen20.2))\n",
    "#n20.3.outsample <- caret::confusionMatrix(xtabs(~test.y + yhat_unseen20.3))\n",
    "#n20.4.outsample <- caret::confusionMatrix(xtabs(~test.y + yhat_unseen20.4))\n",
    "#n20.5.outsample <- caret::confusionMatrix(xtabs(~test.y + yhat_unseen20.5))\n",
    "#n20.6.outsample <- caret::confusionMatrix(xtabs(~test.y + yhat_unseen20.6))\n",
    "#n20.7.outsample <- caret::confusionMatrix(xtabs(~test.y + yhat_unseen20.7))\n",
    "#n20.8.outsample <- caret::confusionMatrix(xtabs(~test.y + yhat_unseen20.8))\n",
    "#n20.9.outsample <- caret::confusionMatrix(xtabs(~test.y + yhat_unseen20.9))\n",
    "#n20.10.outsample <- caret::confusionMatrix(xtabs(~test.y + yhat_unseen20.10))\n",
    "\n",
    "#shrinkage <- rbind(\n",
    "#  cbind(Size = 20.0, Sample = \"In\", as.data.frame(t(n20.0.insample$overall[measures]))),\n",
    "#  cbind(Size = 20.0, Sample = \"Out\", as.data.frame(t(n20.0.outsample$overall[measures]))),\n",
    "#  cbind(Size = 20.1, Sample = \"In\", as.data.frame(t(n20.1.insample$overall[measures]))),\n",
    "#  cbind(Size = 20.1, Sample = \"Out\", as.data.frame(t(n20.1.outsample$overall[measures]))),\n",
    "#  cbind(Size = 20.2, Sample = \"In\", as.data.frame(t(n20.2.insample$overall[measures]))),\n",
    "#  cbind(Size = 20.2, Sample = \"Out\", as.data.frame(t(n20.2.outsample$overall[measures]))),\n",
    "#  cbind(Size = 20.3, Sample = \"In\", as.data.frame(t(n20.3.insample$overall[measures]))),\n",
    "#  cbind(Size = 20.3, Sample = \"Out\", as.data.frame(t(n20.3.outsample$overall[measures]))),\n",
    "#  cbind(Size = 20.4, Sample = \"In\", as.data.frame(t(n20.4.insample$overall[measures]))),\n",
    "#  cbind(Size = 20.4, Sample = \"Out\", as.data.frame(t(n20.4.outsample$overall[measures]))),\n",
    "#  cbind(Size = 20.5, Sample = \"In\", as.data.frame(t(n20.5.insample$overall[measures]))),\n",
    "#  cbind(Size = 20.5, Sample = \"Out\", as.data.frame(t(n20.5.outsample$overall[measures]))),\n",
    "#  cbind(Size = 20.6, Sample = \"In\", as.data.frame(t(n20.6.insample$overall[measures]))),\n",
    "#  cbind(Size = 20.6, Sample = \"Out\", as.data.frame(t(n20.6.outsample$overall[measures]))),\n",
    "#  cbind(Size = 20.7, Sample = \"In\", as.data.frame(t(n20.7.insample$overall[measures]))),\n",
    "#  cbind(Size = 20.7, Sample = \"Out\", as.data.frame(t(n20.7.outsample$overall[measures]))),\n",
    "#  cbind(Size = 20.8, Sample = \"In\", as.data.frame(t(n20.8.insample$overall[measures]))),\n",
    "#  cbind(Size = 20.8, Sample = \"Out\", as.data.frame(t(n20.8.outsample$overall[measures]))),\n",
    "#  cbind(Size = 20.9, Sample = \"In\", as.data.frame(t(n20.9.insample$overall[measures]))),\n",
    "#  cbind(Size = 20.9, Sample = \"Out\", as.data.frame(t(n20.9.outsample$overall[measures]))),\n",
    "#  cbind(Size = 20.99, Sample = \"In\", as.data.frame(t(n20.10.insample$overall[measures]))),\n",
    "#  cbind(Size = 20.99, Sample = \"Out\", as.data.frame(t(n20.10.outsample$overall[measures])))\n",
    "#  )\n",
    "#shrinkage$Pkg <- rep(c(\"In\", \"Out\"), 1)\n",
    "\n",
    "#dodge <- position_dodge(width=0.4)\n",
    "\n",
    "#p.shrinkage <- ggplot(shrinkage, aes(interaction(Size, sep = \" : \"), Accuracy,\n",
    "#                      ymin = AccuracyLower, ymax = AccuracyUpper,\n",
    "#                      shape = Sample, linetype = Sample)) +\n",
    "#  geom_point(size = 2.5, position = dodge) +\n",
    "#  geom_errorbar(width = .25, position = dodge) +\n",
    "#  xlab(\"\") + ylab(\"Accuracy + 95% CI\") +\n",
    "#  theme_classic() +\n",
    "#  theme(legend.key.size = unit(1, \"cm\"), legend.position = c(.8, .2))\n",
    "\n",
    "#png(\"test_20.png\",\n",
    "#    width = 6, height = 6, units = \"in\", res = 600)\n",
    "#  print(p.shrinkage)\n",
    "#dev.off()\n",
    "\n",
    "#m5 <- train(x = train.X, y = train.y,\n",
    "#           method = \"nnet\", \n",
    "#           tuneGrid = expand.grid(\n",
    "#           .size = c(5),\n",
    "#           .decay = 0),\n",
    "#           trControl= trainControl(method = \"none\"),\n",
    "#           MaxNWts = 2000,\n",
    "#           maxit = 100)\n",
    "#m10 <- train(x = train.X, y = train.y,\n",
    "#           method = \"nnet\", \n",
    "#           tuneGrid = expand.grid(\n",
    "#           .size = c(10),\n",
    "#           .decay = 0),\n",
    "#           trControl= trainControl(method = \"none\"),\n",
    "#           MaxNWts = 2000,\n",
    "#           maxit = 100)\n",
    "#m15 <- train(x = train.X, y = train.y,\n",
    "#           method = \"nnet\", \n",
    "#           tuneGrid = expand.grid(\n",
    "#           .size = c(15),\n",
    "#           .decay = 0),\n",
    "#           trControl= trainControl(method = \"none\"),\n",
    "#           MaxNWts = 2000,\n",
    "#           maxit = 100)\n",
    "#m20 <- train(x = train.X, y = train.y,\n",
    "#           method = \"nnet\", \n",
    "#           tuneGrid = expand.grid(\n",
    "#           .size = c(20),\n",
    "#           .decay = 0),\n",
    "#           trControl= trainControl(method = \"none\"),\n",
    "#           MaxNWts = 2000,\n",
    "#           maxit = 100)\n",
    "#m30 <- train(x = train.X, y = train.y,\n",
    "#           method = \"nnet\", \n",
    "#           tuneGrid = expand.grid(\n",
    "#           .size = c(30),\n",
    "#           .decay = 0),\n",
    "#           trControl= trainControl(method = \"none\"),\n",
    "#           MaxNWts = 2000,\n",
    "#           maxit = 100)\n",
    "#m40 <- train(x = train.X, y = train.y,\n",
    "#           method = \"nnet\", \n",
    "#           tuneGrid = expand.grid(\n",
    "#           .size = c(40),\n",
    "#           .decay = 0),\n",
    "#           trControl= trainControl(method = \"none\"),\n",
    "#           MaxNWts = 2000,\n",
    "#           maxit = 100)\n",
    "#m50 <- train(x = train.X, y = train.y,\n",
    "#           method = \"nnet\", \n",
    "#           tuneGrid = expand.grid(\n",
    "#           .size = c(50),\n",
    "#           .decay = 0),\n",
    "#           trControl= trainControl(method = \"none\"),\n",
    "#           MaxNWts = 2000,\n",
    "#           maxit = 100)\n",
    "#m70 <- train(x = train.X, y = train.y,\n",
    "#           method = \"nnet\", \n",
    "#           tuneGrid = expand.grid(\n",
    "#           .size = c(70),\n",
    "#           .decay = 0),\n",
    "#           trControl= trainControl(method = \"none\"),\n",
    "#           MaxNWts = 2000,\n",
    "#           maxit = 100)\n",
    "#m100 <- train(x = train.X, y = train.y,\n",
    "#           method = \"nnet\", \n",
    "#           tuneGrid = expand.grid(\n",
    "#           .size = c(100),\n",
    "#           .decay = 0),\n",
    "#           trControl= trainControl(method = \"none\"),\n",
    "#           MaxNWts = 20000,\n",
    "#           maxit = 100)\n",
    "\n",
    "#yhat5 <- predict(m5)\n",
    "#yhat10 <- predict(m10)\n",
    "#yhat15 <- predict(m15)\n",
    "#yhat20 <- predict(m20)\n",
    "#yhat30 <- predict(m30)\n",
    "#yhat40 <- predict(m40)\n",
    "#yhat50 <- predict(m50)\n",
    "#yhat70 <- predict(m70)\n",
    "#yhat100 <- predict(m100)\n",
    "\n",
    "#yhat_unseen5 <- predict(m5, as.matrix(test.X))\n",
    "#yhat_unseen10 <- predict(m10, as.matrix(test.X))\n",
    "#yhat_unseen15 <- predict(m15, as.matrix(test.X))\n",
    "#yhat_unseen20 <- predict(m20, as.matrix(test.X))\n",
    "#yhat_unseen30 <- predict(m30, as.matrix(test.X))\n",
    "#yhat_unseen40 <- predict(m40, as.matrix(test.X))\n",
    "#yhat_unseen50 <- predict(m50, as.matrix(test.X))\n",
    "#yhat_unseen70 <- predict(m70, as.matrix(test.X))\n",
    "#yhat_unseen100 <- predict(m100, as.matrix(test.X))\n",
    "\n",
    "#measures <- c(\"AccuracyNull\", \"Accuracy\", \"AccuracyLower\", \"AccuracyUpper\")\n",
    "\n",
    "#n5.insample <- caret::confusionMatrix(xtabs(~train.y + yhat5))\n",
    "#n10.insample <- caret::confusionMatrix(xtabs(~train.y + yhat10))\n",
    "#n15.insample <- caret::confusionMatrix(xtabs(~train.y + yhat15))\n",
    "#n20.insample <- caret::confusionMatrix(xtabs(~train.y + yhat20))\n",
    "#n30.insample <- caret::confusionMatrix(xtabs(~train.y + yhat30))\n",
    "#n40.insample <- caret::confusionMatrix(xtabs(~train.y + yhat40))\n",
    "#n50.insample <- caret::confusionMatrix(xtabs(~train.y + yhat50))\n",
    "#n70.insample <- caret::confusionMatrix(xtabs(~train.y + yhat70))\n",
    "#n100.insample <- caret::confusionMatrix(xtabs(~train.y + yhat100))\n",
    "\n",
    "#n5.outsample <- caret::confusionMatrix(xtabs(~test.y + yhat_unseen5))\n",
    "#n10.outsample <- caret::confusionMatrix(xtabs(~test.y + yhat_unseen10))\n",
    "#n15.outsample <- caret::confusionMatrix(xtabs(~test.y + yhat_unseen15))\n",
    "#n20.outsample <- caret::confusionMatrix(xtabs(~test.y + yhat_unseen20))\n",
    "#n30.outsample <- caret::confusionMatrix(xtabs(~test.y + yhat_unseen30))\n",
    "#n40.outsample <- caret::confusionMatrix(xtabs(~test.y + yhat_unseen40))\n",
    "#n50.outsample <- caret::confusionMatrix(xtabs(~test.y + yhat_unseen50))\n",
    "#n70.outsample <- caret::confusionMatrix(xtabs(~test.y + yhat_unseen70))\n",
    "#n100.outsample <- caret::confusionMatrix(xtabs(~test.y + yhat_unseen100))\n",
    "\n",
    "#shrinkage <- rbind(\n",
    "#  cbind(Size = 5, Sample = \"In\", as.data.frame(t(n5.insample$overall[measures]))),\n",
    "#  cbind(Size = 5, Sample = \"Out\", as.data.frame(t(n5.outsample$overall[measures]))),\n",
    "#  cbind(Size = 10, Sample = \"In\", as.data.frame(t(n10.insample$overall[measures]))),\n",
    "#  cbind(Size = 10, Sample = \"Out\", as.data.frame(t(n10.outsample$overall[measures]))),\n",
    "#  cbind(Size = 15, Sample = \"In\", as.data.frame(t(n15.insample$overall[measures]))),\n",
    "#  cbind(Size = 15, Sample = \"Out\", as.data.frame(t(n15.outsample$overall[measures]))),\n",
    "#  cbind(Size = 20, Sample = \"In\", as.data.frame(t(n20.insample$overall[measures]))),\n",
    "#  cbind(Size = 20, Sample = \"Out\", as.data.frame(t(n20.outsample$overall[measures]))),\n",
    "#  cbind(Size = 30, Sample = \"In\", as.data.frame(t(n30.insample$overall[measures]))),\n",
    "#  cbind(Size = 30, Sample = \"Out\", as.data.frame(t(n30.outsample$overall[measures]))),\n",
    "#  cbind(Size = 40, Sample = \"In\", as.data.frame(t(n40.insample$overall[measures]))),\n",
    "#  cbind(Size = 40, Sample = \"Out\", as.data.frame(t(n40.outsample$overall[measures]))),\n",
    "#  cbind(Size = 50, Sample = \"In\", as.data.frame(t(n50.insample$overall[measures]))),\n",
    "#  cbind(Size = 50, Sample = \"Out\", as.data.frame(t(n50.outsample$overall[measures]))),\n",
    "#  cbind(Size = 70, Sample = \"In\", as.data.frame(t(n70.insample$overall[measures]))),\n",
    "#  cbind(Size = 70, Sample = \"Out\", as.data.frame(t(n70.outsample$overall[measures]))),\n",
    "#  cbind(Size = 100, Sample = \"In\", as.data.frame(t(n100.insample$overall[measures]))),\n",
    "#  cbind(Size = 100, Sample = \"Out\", as.data.frame(t(n100.outsample$overall[measures])))\n",
    "#  )\n",
    "#shrinkage$Pkg <- rep(c(\"In\", \"Out\"), 1)\n",
    "\n",
    "#dodge <- position_dodge(width=0.4)\n",
    "\n",
    "#p.shrinkage <- ggplot(shrinkage, aes(interaction(Size, sep = \" : \"), Accuracy,\n",
    "#                      ymin = AccuracyLower, ymax = AccuracyUpper,\n",
    "#                      shape = Sample, linetype = Sample)) +\n",
    "#  geom_point(size = 2.5, position = dodge) +\n",
    "#  geom_errorbar(width = .25, position = dodge) +\n",
    "#  xlab(\"\") + ylab(\"Accuracy + 95% CI\") +\n",
    "#  theme_classic() +\n",
    "#  theme(legend.key.size = unit(1, \"cm\"), legend.position = c(.8, .2))\n",
    "\n",
    "#png(\"test_5150.png\",\n",
    "#    width = 6, height = 6, units = \"in\", res = 600)\n",
    "#  print(p.shrinkage)\n",
    "#dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "H2O is not running yet, starting it now...\n",
      "\n",
      "Note:  In case of errors look at the following log files:\n",
      "    C:\\Users\\laris\\AppData\\Local\\Temp\\RtmpCenL01/h2o_laris_started_from_r.out\n",
      "    C:\\Users\\laris\\AppData\\Local\\Temp\\RtmpCenL01/h2o_laris_started_from_r.err\n",
      "\n",
      "\n",
      "Starting H2O JVM and connecting: .. Connection successful!\n",
      "\n",
      "R is connected to the H2O cluster: \n",
      "    H2O cluster uptime:         6 seconds 558 milliseconds \n",
      "    H2O cluster timezone:       Europe/Berlin \n",
      "    H2O data parsing timezone:  UTC \n",
      "    H2O cluster version:        3.18.0.8 \n",
      "    H2O cluster version age:    1 month and 17 days  \n",
      "    H2O cluster name:           H2O_started_from_R_laris_dzq933 \n",
      "    H2O cluster total nodes:    1 \n",
      "    H2O cluster total memory:   1.66 GB \n",
      "    H2O cluster total cores:    4 \n",
      "    H2O cluster allowed cores:  4 \n",
      "    H2O cluster healthy:        TRUE \n",
      "    H2O Connection ip:          localhost \n",
      "    H2O Connection port:        54321 \n",
      "    H2O Connection proxy:       NA \n",
      "    H2O Internal Security:      FALSE \n",
      "    H2O API Extensions:         Algos, AutoML, Core V3, Core V4 \n",
      "    R Version:                  R version 3.4.1 (2017-06-30) \n",
      "\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n"
     ]
    }
   ],
   "source": [
    "c1 <- h2o.init()\n",
    "\n",
    "train$volact <- as.factor(train$volact)\n",
    "test$volact <- as.factor(test$volact)\n",
    "\n",
    "h2o.train <- as.h2o(train)\n",
    "h2o.test <- as.h2o(test)\n",
    "\n",
    "train.bal$volact <- as.factor(train.bal$volact)\n",
    "h2o.train.bal <- as.h2o(train.bal)\n",
    "\n",
    "xnames <- colnames(train[,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |======================================================================| 100%\n",
      "Model Details:\n",
      "==============\n",
      "\n",
      "H2OBinomialModel: deeplearning\n",
      "Model Key:  DeepLearning_model_R_1528213959851_1 \n",
      "Status of Neuron Layers: predicting volact, 2-class classification, bernoulli distribution, CrossEntropy loss, 65.102 weights/biases, 776,2 KB, 878.466 training samples, mini-batch size 1\n",
      "  layer units      type dropout       l1       l2 mean_rate rate_rms momentum\n",
      "1     1    22     Input  0.00 %                                              \n",
      "2     2   200 Rectifier  0.00 % 0.000000 0.000000  0.031329 0.013669 0.000000\n",
      "3     3   200 Rectifier  0.00 % 0.000000 0.000000  0.033363 0.020096 0.000000\n",
      "4     4   100 Rectifier  0.00 % 0.000000 0.000000  0.026521 0.051848 0.000000\n",
      "5     5     2   Softmax         0.000000 0.000000  0.004574 0.001701 0.000000\n",
      "  mean_weight weight_rms mean_bias bias_rms\n",
      "1                                          \n",
      "2    0.006868   0.222333  0.038202 0.191062\n",
      "3   -0.030958   0.090977  0.897484 0.184012\n",
      "4   -0.013153   0.086897  0.857040 0.063966\n",
      "5    0.003307   0.488869 -0.020956 0.045313\n",
      "\n",
      "H2OBinomialMetrics: deeplearning\n",
      "** Reported on training data. **\n",
      "** Metrics reported on temporary training frame with 9992 samples **\n",
      "\n",
      "MSE:  0.1980177486\n",
      "RMSE:  0.4449918523\n",
      "LogLoss:  0.5794769606\n",
      "Mean Per-Class Error:  0.3524663585\n",
      "AUC:  0.7131651834\n",
      "Gini:  0.4263303669\n",
      "\n",
      "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
      "          0    1    Error        Rate\n",
      "0      3326 3179 0.488701  =3179/6505\n",
      "1       754 2733 0.216232   =754/3487\n",
      "Totals 4080 5912 0.393615  =3933/9992\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "                        metric threshold    value idx\n",
      "1                       max f1  0.307001 0.581551 249\n",
      "2                       max f2  0.146905 0.742172 345\n",
      "3                 max f0point5  0.451654 0.539740 155\n",
      "4                 max accuracy  0.525069 0.699560 115\n",
      "5                max precision  0.924761 1.000000   0\n",
      "6                   max recall  0.035411 1.000000 395\n",
      "7              max specificity  0.924761 1.000000   0\n",
      "8             max absolute_mcc  0.384467 0.298692 199\n",
      "9   max min_per_class_accuracy  0.369972 0.652710 207\n",
      "10 max mean_per_class_accuracy  0.378969 0.654796 202\n",
      "\n",
      "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
      "H2OBinomialMetrics: deeplearning\n",
      "** Reported on validation data. **\n",
      "** Metrics reported on full validation frame **\n",
      "\n",
      "MSE:  0.2030883412\n",
      "RMSE:  0.4506532383\n",
      "LogLoss:  0.5913709148\n",
      "Mean Per-Class Error:  0.352944309\n",
      "AUC:  0.7007148294\n",
      "Gini:  0.4014296588\n",
      "\n",
      "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
      "          0    1    Error        Rate\n",
      "0      3197 3077 0.490437  =3077/6274\n",
      "1       739 2691 0.215452   =739/3430\n",
      "Totals 3936 5768 0.393240  =3816/9704\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "                        metric threshold    value idx\n",
      "1                       max f1  0.304624 0.585127 242\n",
      "2                       max f2  0.163419 0.739771 329\n",
      "3                 max f0point5  0.468836 0.530973 134\n",
      "4                 max accuracy  0.472071 0.683533 132\n",
      "5                max precision  0.846022 1.000000   0\n",
      "6                   max recall  0.041036 1.000000 392\n",
      "7              max specificity  0.846022 1.000000   0\n",
      "8             max absolute_mcc  0.308673 0.287030 239\n",
      "9   max min_per_class_accuracy  0.368882 0.646064 198\n",
      "10 max mean_per_class_accuracy  0.356052 0.649448 207\n",
      "\n",
      "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
      "\n",
      "\n",
      "Scoring History: \n",
      "            timestamp          duration training_speed   epochs iterations\n",
      "1 2018-06-05 18:01:35         0.000 sec                 0.00000          0\n",
      "2 2018-06-05 18:01:45        13.794 sec   4682 obs/sec  0.53109          1\n",
      "3 2018-06-05 18:02:14        43.991 sec   6317 obs/sec  2.64964          5\n",
      "4 2018-06-05 18:02:49  1 min 17.197 sec   7468 obs/sec  5.82655         11\n",
      "5 2018-06-05 18:03:10  1 min 38.807 sec   7879 obs/sec  7.94279         15\n",
      "6 2018-06-05 18:03:32  1 min 59.860 sec   8169 obs/sec 10.05939         19\n",
      "7 2018-06-05 18:03:33  2 min  1.605 sec   8165 obs/sec 10.05939         19\n",
      "        samples training_rmse training_logloss training_auc training_lift\n",
      "1      0.000000                                                          \n",
      "2  46379.000000       0.45854          0.61301      0.69654       2.29240\n",
      "3 231388.000000       0.44875          0.58795      0.70439       2.29240\n",
      "4 508821.000000       0.44499          0.57948      0.71317       2.37837\n",
      "5 693628.000000       0.44236          0.57366      0.72225       2.34971\n",
      "6 878466.000000       0.43971          0.56826      0.73073       2.46433\n",
      "7 878466.000000       0.44499          0.57948      0.71317       2.37837\n",
      "  training_classification_error validation_rmse validation_logloss\n",
      "1                                                                 \n",
      "2                       0.40492         0.46221            0.62111\n",
      "3                       0.40302         0.45206            0.59563\n",
      "4                       0.39361         0.45065            0.59137\n",
      "5                       0.37670         0.45158            0.59352\n",
      "6                       0.38100         0.45158            0.59669\n",
      "7                       0.39361         0.45065            0.59137\n",
      "  validation_auc validation_lift validation_classification_error\n",
      "1                                                               \n",
      "2        0.69239         2.19404                         0.39417\n",
      "3        0.70010         2.30951                         0.38922\n",
      "4        0.70071         2.22291                         0.39324\n",
      "5        0.69689         2.25178                         0.42158\n",
      "6        0.69981         2.22291                         0.37953\n",
      "7        0.70071         2.22291                         0.39324\n",
      "\n",
      "Variable Importances: (Extract with `h2o.varimp`) \n",
      "=================================================\n",
      "\n",
      "Variable Importances: \n",
      "      variable relative_importance scaled_importance percentage\n",
      "1       round1            1.000000          1.000000   0.068141\n",
      "2       eduyrs            0.940406          0.940406   0.064081\n",
      "3        yrbrn            0.852686          0.852686   0.058103\n",
      "4 social_trust            0.837309          0.837309   0.057055\n",
      "5      sclmeet            0.784120          0.784120   0.053431\n",
      "\n",
      "---\n",
      "           variable relative_importance scaled_importance percentage\n",
      "17       solidarity            0.556531          0.556531   0.037923\n",
      "18          domicil            0.553661          0.553661   0.037727\n",
      "19           trstep            0.533433          0.533433   0.036349\n",
      "20 self_realisation            0.492989          0.492989   0.033593\n",
      "21        tolerance            0.482716          0.482716   0.032893\n",
      "22      houseperson            0.382046          0.382046   0.026033\n",
      "  |======================================================================| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.60717230008244"
      ],
      "text/latex": [
       "0.60717230008244"
      ],
      "text/markdown": [
       "0.60717230008244"
      ],
      "text/plain": [
       "[1] 0.6071723001"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.585020683649031"
      ],
      "text/latex": [
       "0.585020683649031"
      ],
      "text/markdown": [
       "0.585020683649031"
      ],
      "text/plain": [
       "[1] 0.5850206836"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>variable</th><th scope=col>relative_importance</th><th scope=col>scaled_importance</th><th scope=col>percentage</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>round1            </td><td>1.0000000000      </td><td>1.0000000000      </td><td>0.06814146490     </td></tr>\n",
       "\t<tr><td>eduyrs            </td><td>0.9404057264      </td><td>0.9404057264      </td><td>0.06408062380     </td></tr>\n",
       "\t<tr><td>yrbrn             </td><td>0.8526864648      </td><td>0.8526864648      </td><td>0.05810330481     </td></tr>\n",
       "\t<tr><td>social_trust      </td><td>0.8373092413      </td><td>0.8373092413      </td><td>0.05705547828     </td></tr>\n",
       "\t<tr><td>sclmeet           </td><td>0.7841196656      </td><td>0.7841196656      </td><td>0.05343106268     </td></tr>\n",
       "\t<tr><td>church            </td><td>0.7663334608      </td><td>0.7663334608      </td><td>0.05221908462     </td></tr>\n",
       "\t<tr><td>trust_exe         </td><td>0.7480811477      </td><td>0.7480811477      </td><td>0.05097534527     </td></tr>\n",
       "\t<tr><td>female            </td><td>0.6930943131      </td><td>0.6930943131      </td><td>0.04722846181     </td></tr>\n",
       "\t<tr><td>children          </td><td>0.6849380732      </td><td>0.6849380732      </td><td>0.04667268367     </td></tr>\n",
       "\t<tr><td>political_interest</td><td>0.6819077730      </td><td>0.6819077730      </td><td>0.04646619458     </td></tr>\n",
       "\t<tr><td>trust_leg         </td><td>0.6545783281      </td><td>0.6545783281      </td><td>0.04460392617     </td></tr>\n",
       "\t<tr><td>tvtot             </td><td>0.6311377883      </td><td>0.6311377883      </td><td>0.04300665345     </td></tr>\n",
       "\t<tr><td>tvpol             </td><td>0.6186070442      </td><td>0.6186070442      </td><td>0.04215279019     </td></tr>\n",
       "\t<tr><td>wkhtot            </td><td>0.5979806781      </td><td>0.5979806781      </td><td>0.04074727939     </td></tr>\n",
       "\t<tr><td>married           </td><td>0.5961201191      </td><td>0.5961201191      </td><td>0.04062049817     </td></tr>\n",
       "\t<tr><td>stfdem            </td><td>0.5866748095      </td><td>0.5866748095      </td><td>0.03997688094     </td></tr>\n",
       "\t<tr><td>solidarity        </td><td>0.5565313697      </td><td>0.5565313697      </td><td>0.03792286280     </td></tr>\n",
       "\t<tr><td>domicil           </td><td>0.5536606908      </td><td>0.5536606908      </td><td>0.03772725053     </td></tr>\n",
       "\t<tr><td>trstep            </td><td>0.5334332585      </td><td>0.5334332585      </td><td>0.03634892367     </td></tr>\n",
       "\t<tr><td>self_realisation  </td><td>0.4929893911      </td><td>0.4929893911      </td><td>0.03359301929     </td></tr>\n",
       "\t<tr><td>tolerance         </td><td>0.4827164710      </td><td>0.4827164710      </td><td>0.03289300746     </td></tr>\n",
       "\t<tr><td>houseperson       </td><td>0.3820464313      </td><td>0.3820464313      </td><td>0.02603320349     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " variable & relative\\_importance & scaled\\_importance & percentage\\\\\n",
       "\\hline\n",
       "\t round1             & 1.0000000000       & 1.0000000000       & 0.06814146490     \\\\\n",
       "\t eduyrs             & 0.9404057264       & 0.9404057264       & 0.06408062380     \\\\\n",
       "\t yrbrn              & 0.8526864648       & 0.8526864648       & 0.05810330481     \\\\\n",
       "\t social\\_trust       & 0.8373092413         & 0.8373092413         & 0.05705547828       \\\\\n",
       "\t sclmeet            & 0.7841196656       & 0.7841196656       & 0.05343106268     \\\\\n",
       "\t church             & 0.7663334608       & 0.7663334608       & 0.05221908462     \\\\\n",
       "\t trust\\_exe          & 0.7480811477         & 0.7480811477         & 0.05097534527       \\\\\n",
       "\t female             & 0.6930943131       & 0.6930943131       & 0.04722846181     \\\\\n",
       "\t children           & 0.6849380732       & 0.6849380732       & 0.04667268367     \\\\\n",
       "\t political\\_interest & 0.6819077730         & 0.6819077730         & 0.04646619458       \\\\\n",
       "\t trust\\_leg          & 0.6545783281         & 0.6545783281         & 0.04460392617       \\\\\n",
       "\t tvtot              & 0.6311377883       & 0.6311377883       & 0.04300665345     \\\\\n",
       "\t tvpol              & 0.6186070442       & 0.6186070442       & 0.04215279019     \\\\\n",
       "\t wkhtot             & 0.5979806781       & 0.5979806781       & 0.04074727939     \\\\\n",
       "\t married            & 0.5961201191       & 0.5961201191       & 0.04062049817     \\\\\n",
       "\t stfdem             & 0.5866748095       & 0.5866748095       & 0.03997688094     \\\\\n",
       "\t solidarity         & 0.5565313697       & 0.5565313697       & 0.03792286280     \\\\\n",
       "\t domicil            & 0.5536606908       & 0.5536606908       & 0.03772725053     \\\\\n",
       "\t trstep             & 0.5334332585       & 0.5334332585       & 0.03634892367     \\\\\n",
       "\t self\\_realisation   & 0.4929893911         & 0.4929893911         & 0.03359301929       \\\\\n",
       "\t tolerance          & 0.4827164710       & 0.4827164710       & 0.03289300746     \\\\\n",
       "\t houseperson        & 0.3820464313       & 0.3820464313       & 0.02603320349     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "variable | relative_importance | scaled_importance | percentage | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| round1             | 1.0000000000       | 1.0000000000       | 0.06814146490      | \n",
       "| eduyrs             | 0.9404057264       | 0.9404057264       | 0.06408062380      | \n",
       "| yrbrn              | 0.8526864648       | 0.8526864648       | 0.05810330481      | \n",
       "| social_trust       | 0.8373092413       | 0.8373092413       | 0.05705547828      | \n",
       "| sclmeet            | 0.7841196656       | 0.7841196656       | 0.05343106268      | \n",
       "| church             | 0.7663334608       | 0.7663334608       | 0.05221908462      | \n",
       "| trust_exe          | 0.7480811477       | 0.7480811477       | 0.05097534527      | \n",
       "| female             | 0.6930943131       | 0.6930943131       | 0.04722846181      | \n",
       "| children           | 0.6849380732       | 0.6849380732       | 0.04667268367      | \n",
       "| political_interest | 0.6819077730       | 0.6819077730       | 0.04646619458      | \n",
       "| trust_leg          | 0.6545783281       | 0.6545783281       | 0.04460392617      | \n",
       "| tvtot              | 0.6311377883       | 0.6311377883       | 0.04300665345      | \n",
       "| tvpol              | 0.6186070442       | 0.6186070442       | 0.04215279019      | \n",
       "| wkhtot             | 0.5979806781       | 0.5979806781       | 0.04074727939      | \n",
       "| married            | 0.5961201191       | 0.5961201191       | 0.04062049817      | \n",
       "| stfdem             | 0.5866748095       | 0.5866748095       | 0.03997688094      | \n",
       "| solidarity         | 0.5565313697       | 0.5565313697       | 0.03792286280      | \n",
       "| domicil            | 0.5536606908       | 0.5536606908       | 0.03772725053      | \n",
       "| trstep             | 0.5334332585       | 0.5334332585       | 0.03634892367      | \n",
       "| self_realisation   | 0.4929893911       | 0.4929893911       | 0.03359301929      | \n",
       "| tolerance          | 0.4827164710       | 0.4827164710       | 0.03289300746      | \n",
       "| houseperson        | 0.3820464313       | 0.3820464313       | 0.02603320349      | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   variable           relative_importance scaled_importance percentage   \n",
       "1  round1             1.0000000000        1.0000000000      0.06814146490\n",
       "2  eduyrs             0.9404057264        0.9404057264      0.06408062380\n",
       "3  yrbrn              0.8526864648        0.8526864648      0.05810330481\n",
       "4  social_trust       0.8373092413        0.8373092413      0.05705547828\n",
       "5  sclmeet            0.7841196656        0.7841196656      0.05343106268\n",
       "6  church             0.7663334608        0.7663334608      0.05221908462\n",
       "7  trust_exe          0.7480811477        0.7480811477      0.05097534527\n",
       "8  female             0.6930943131        0.6930943131      0.04722846181\n",
       "9  children           0.6849380732        0.6849380732      0.04667268367\n",
       "10 political_interest 0.6819077730        0.6819077730      0.04646619458\n",
       "11 trust_leg          0.6545783281        0.6545783281      0.04460392617\n",
       "12 tvtot              0.6311377883        0.6311377883      0.04300665345\n",
       "13 tvpol              0.6186070442        0.6186070442      0.04215279019\n",
       "14 wkhtot             0.5979806781        0.5979806781      0.04074727939\n",
       "15 married            0.5961201191        0.5961201191      0.04062049817\n",
       "16 stfdem             0.5866748095        0.5866748095      0.03997688094\n",
       "17 solidarity         0.5565313697        0.5565313697      0.03792286280\n",
       "18 domicil            0.5536606908        0.5536606908      0.03772725053\n",
       "19 trstep             0.5334332585        0.5334332585      0.03634892367\n",
       "20 self_realisation   0.4929893911        0.4929893911      0.03359301929\n",
       "21 tolerance          0.4827164710        0.4827164710      0.03289300746\n",
       "22 houseperson        0.3820464313        0.3820464313      0.02603320349"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |======================================================================| 100%\n",
      "Model Details:\n",
      "==============\n",
      "\n",
      "H2OBinomialModel: deeplearning\n",
      "Model Key:  DeepLearning_model_R_1528213959851_15 \n",
      "Status of Neuron Layers: predicting volact, 2-class classification, bernoulli distribution, CrossEntropy loss, 65.102 weights/biases, 776,2 KB, 1.163.056 training samples, mini-batch size 1\n",
      "  layer units      type dropout       l1       l2 mean_rate rate_rms momentum\n",
      "1     1    22     Input  0.00 %                                              \n",
      "2     2   200 Rectifier  0.00 % 0.000000 0.000000  0.004387 0.001945 0.000000\n",
      "3     3   200 Rectifier  0.00 % 0.000000 0.000000  0.005133 0.004011 0.000000\n",
      "4     4   100 Rectifier  0.00 % 0.000000 0.000000  0.037698 0.050822 0.000000\n",
      "5     5     2   Softmax         0.000000 0.000000  0.004762 0.002541 0.000000\n",
      "  mean_weight weight_rms mean_bias bias_rms\n",
      "1                                          \n",
      "2    0.036463   0.255269 -0.045365 0.158897\n",
      "3   -0.043728   0.105788  0.818197 0.196209\n",
      "4   -0.016813   0.102464 -0.356267 0.494156\n",
      "5    0.082764   0.437465  0.029016 0.108099\n",
      "\n",
      "H2OBinomialMetrics: deeplearning\n",
      "** Reported on training data. **\n",
      "** Metrics reported on temporary training frame with 9992 samples **\n",
      "\n",
      "MSE:  0.217626222\n",
      "RMSE:  0.4665042572\n",
      "LogLoss:  0.6188199529\n",
      "Mean Per-Class Error:  0.3002719612\n",
      "AUC:  0.78528725\n",
      "Gini:  0.5705745001\n",
      "\n",
      "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
      "          0    1    Error        Rate\n",
      "0      2500 2520 0.501992  =2520/5020\n",
      "1       490 4482 0.098552   =490/4972\n",
      "Totals 2990 7002 0.301241  =3010/9992\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "                        metric threshold    value idx\n",
      "1                       max f1  0.217326 0.748622 293\n",
      "2                       max f2  0.107622 0.859762 345\n",
      "3                 max f0point5  0.357090 0.709604 215\n",
      "4                 max accuracy  0.275722 0.710969 261\n",
      "5                max precision  0.997177 1.000000   0\n",
      "6                   max recall  0.005654 1.000000 397\n",
      "7              max specificity  0.997177 1.000000   0\n",
      "8             max absolute_mcc  0.217326 0.436154 293\n",
      "9   max min_per_class_accuracy  0.335627 0.706175 226\n",
      "10 max mean_per_class_accuracy  0.275722 0.711505 261\n",
      "\n",
      "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
      "H2OBinomialMetrics: deeplearning\n",
      "** Reported on validation data. **\n",
      "** Metrics reported on full validation frame **\n",
      "\n",
      "MSE:  0.217341589\n",
      "RMSE:  0.4661990873\n",
      "LogLoss:  0.6492591559\n",
      "Mean Per-Class Error:  0.3879315905\n",
      "AUC:  0.6695788812\n",
      "Gini:  0.3391577625\n",
      "\n",
      "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
      "          0    1    Error        Rate\n",
      "0      2822 3452 0.550207  =3452/6274\n",
      "1       774 2656 0.225656   =774/3430\n",
      "Totals 3596 6108 0.435491  =4226/9704\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "                        metric threshold    value idx\n",
      "1                       max f1  0.209094 0.556930 280\n",
      "2                       max f2  0.020977 0.734629 387\n",
      "3                 max f0point5  0.405695 0.511731 164\n",
      "4                 max accuracy  0.494838 0.671270 117\n",
      "5                max precision  0.910730 0.769231   4\n",
      "6                   max recall  0.001247 1.000000 399\n",
      "7              max specificity  0.966777 0.999841   0\n",
      "8             max absolute_mcc  0.405695 0.241784 164\n",
      "9   max min_per_class_accuracy  0.307127 0.623032 221\n",
      "10 max mean_per_class_accuracy  0.272450 0.625605 243\n",
      "\n",
      "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
      "\n",
      "\n",
      "Scoring History: \n",
      "            timestamp          duration training_speed   epochs iterations\n",
      "1 2018-06-05 18:03:43         0.000 sec                 0.00000          0\n",
      "2 2018-06-05 18:03:51        12.435 sec   3298 obs/sec  0.24268          1\n",
      "3 2018-06-05 18:04:19        39.544 sec   6545 obs/sec  1.93679          8\n",
      "4 2018-06-05 18:04:37        56.980 sec   7240 obs/sec  3.14718         13\n",
      "5 2018-06-05 18:04:53  1 min 12.912 sec   7788 obs/sec  4.36123         18\n",
      "6 2018-06-05 18:05:11  1 min 31.121 sec   8251 obs/sec  5.82003         24\n",
      "7 2018-06-05 18:05:29  1 min 48.937 sec   8586 obs/sec  7.27745         30\n",
      "8 2018-06-05 18:05:47  2 min  6.743 sec   8826 obs/sec  8.73022         36\n",
      "9 2018-06-05 18:06:04  2 min 24.093 sec   9024 obs/sec 10.17867         42\n",
      "         samples training_rmse training_logloss training_auc training_lift\n",
      "1       0.000000                                                          \n",
      "2   27730.000000       0.68942          1.95512      0.66222       1.60772\n",
      "3  221305.000000       0.70540          6.63404      0.52804       1.34647\n",
      "4  359609.000000       0.69196          2.07924      0.67071       1.70821\n",
      "5  498332.000000       0.53898          0.80404      0.71016       1.66801\n",
      "6  665020.000000       0.67019          1.64023      0.69690       1.68811\n",
      "7  831551.000000       0.70878          7.18199      0.69217       1.56753\n",
      "8  997550.000000       0.65336          1.43169      0.76405       1.76850\n",
      "9 1163056.000000       0.46650          0.61882      0.78529       1.84888\n",
      "  training_classification_error validation_rmse validation_logloss\n",
      "1                                                                 \n",
      "2                       0.42364         0.58120            1.39733\n",
      "3                       0.50140         0.59453            4.71941\n",
      "4                       0.42354         0.58351            1.49228\n",
      "5                       0.39151         0.47366            0.65050\n",
      "6                       0.39061         0.56662            1.20671\n",
      "7                       0.36809         0.80405            9.20181\n",
      "8                       0.32946         0.55903            1.12800\n",
      "9                       0.30124         0.46620            0.64926\n",
      "  validation_auc validation_lift validation_classification_error\n",
      "1                                                               \n",
      "2        0.66827         1.96309                         0.47012\n",
      "3        0.51952         1.50118                         0.64138\n",
      "4        0.66268         2.04969                         0.49639\n",
      "5        0.69615         2.16517                         0.40004\n",
      "6        0.67465         2.16517                         0.44157\n",
      "7        0.66925         2.10743                         0.42704\n",
      "8        0.68028         1.84761                         0.43807\n",
      "9        0.66958         1.70327                         0.43549\n",
      "\n",
      "Variable Importances: (Extract with `h2o.varimp`) \n",
      "=================================================\n",
      "\n",
      "Variable Importances: \n",
      "   variable relative_importance scaled_importance percentage\n",
      "1    eduyrs            1.000000          1.000000   0.062173\n",
      "2   sclmeet            0.891595          0.891595   0.055433\n",
      "3    round1            0.888756          0.888756   0.055257\n",
      "4 trust_exe            0.838968          0.838968   0.052161\n",
      "5    church            0.832944          0.832944   0.051787\n",
      "\n",
      "---\n",
      "           variable relative_importance scaled_importance percentage\n",
      "17           stfdem            0.665868          0.665868   0.041399\n",
      "18            tvpol            0.627235          0.627235   0.038997\n",
      "19           trstep            0.620674          0.620674   0.038589\n",
      "20 self_realisation            0.583025          0.583025   0.036248\n",
      "21        tolerance            0.581653          0.581653   0.036163\n",
      "22      houseperson            0.542279          0.542279   0.033715\n",
      "  |======================================================================| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.564715581203627"
      ],
      "text/latex": [
       "0.564715581203627"
      ],
      "text/markdown": [
       "0.564715581203627"
      ],
      "text/plain": [
       "[1] 0.5647155812"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.556581986143187"
      ],
      "text/latex": [
       "0.556581986143187"
      ],
      "text/markdown": [
       "0.556581986143187"
      ],
      "text/plain": [
       "[1] 0.5565819861"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |======================================================================| 100%\n",
      "Model Details:\n",
      "==============\n",
      "\n",
      "H2OBinomialModel: drf\n",
      "Model Key:  forest2 \n",
      "Model Summary: \n",
      "  number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n",
      "1             400                      400            88835568        30\n",
      "  max_depth mean_depth min_leaves max_leaves mean_leaves\n",
      "1        30   30.00000      17286      17981 17691.73800\n",
      "\n",
      "H2OBinomialMetrics: drf\n",
      "** Reported on training data. **\n",
      "** Metrics reported on Out-Of-Bag training samples **\n",
      "\n",
      "MSE:  0.2020346359\n",
      "RMSE:  0.4494826313\n",
      "LogLoss:  0.5895245095\n",
      "Mean Per-Class Error:  0.3652271738\n",
      "AUC:  0.696778645\n",
      "Gini:  0.39355729\n",
      "\n",
      "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
      "           0     1    Error          Rate\n",
      "0      28205 28927 0.506319  =28927/57132\n",
      "1       6768 23428 0.224136   =6768/30196\n",
      "Totals 34973 52355 0.408746  =35695/87328\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "                        metric threshold    value idx\n",
      "1                       max f1  0.307289 0.567601 248\n",
      "2                       max f2  0.167160 0.733022 342\n",
      "3                 max f0point5  0.433119 0.523071 162\n",
      "4                 max accuracy  0.498303 0.686893 123\n",
      "5                max precision  0.818792 0.898305   6\n",
      "6                   max recall  0.014016 1.000000 399\n",
      "7              max specificity  0.915368 0.999982   0\n",
      "8             max absolute_mcc  0.365065 0.275875 209\n",
      "9   max min_per_class_accuracy  0.363672 0.643615 210\n",
      "10 max mean_per_class_accuracy  0.365065 0.644343 209\n",
      "\n",
      "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
      "H2OBinomialMetrics: drf\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE:  0.2021276053\n",
      "RMSE:  0.4495860377\n",
      "LogLoss:  0.5898159816\n",
      "Mean Per-Class Error:  0.3543355846\n",
      "AUC:  0.7057769071\n",
      "Gini:  0.4115538141\n",
      "\n",
      "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
      "          0    1    Error        Rate\n",
      "0      3271 3003 0.478642  =3003/6274\n",
      "1       789 2641 0.230029   =789/3430\n",
      "Totals 4060 5644 0.390767  =3792/9704\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "                        metric threshold    value idx\n",
      "1                       max f1  0.313532 0.582103 240\n",
      "2                       max f2  0.155185 0.740871 354\n",
      "3                 max f0point5  0.402462 0.546920 171\n",
      "4                 max accuracy  0.473044 0.687036 122\n",
      "5                max precision  0.860535 1.000000   0\n",
      "6                   max recall  0.037444 1.000000 398\n",
      "7              max specificity  0.860535 1.000000   0\n",
      "8             max absolute_mcc  0.378329 0.300173 190\n",
      "9   max min_per_class_accuracy  0.361252 0.652375 203\n",
      "10 max mean_per_class_accuracy  0.374358 0.654612 193\n",
      "\n",
      "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
      "\n",
      "\n",
      "Scoring History: \n",
      "             timestamp          duration number_of_trees training_rmse\n",
      "1  2018-06-05 18:06:11         0.172 sec               0              \n",
      "2  2018-06-05 18:06:14         2.588 sec               1       0.63986\n",
      "3  2018-06-05 18:06:15         3.778 sec               2       0.61970\n",
      "4  2018-06-05 18:06:19         7.884 sec               8       0.53519\n",
      "5  2018-06-05 18:06:23        12.143 sec              15       0.49388\n",
      "6  2018-06-05 18:06:27        16.522 sec              22       0.47833\n",
      "7  2018-06-05 18:06:31        20.542 sec              28       0.47108\n",
      "8  2018-06-05 18:06:36        25.064 sec              35       0.46653\n",
      "9  2018-06-05 18:06:42        31.543 sec              45       0.46237\n",
      "10 2018-06-05 18:06:50        39.267 sec              57       0.45919\n",
      "11 2018-06-05 18:06:59        47.927 sec              71       0.45691\n",
      "12 2018-06-05 18:07:09        58.384 sec              88       0.45544\n",
      "13 2018-06-05 18:07:22  1 min 11.102 sec             108       0.45400\n",
      "14 2018-06-05 18:07:42  1 min 31.224 sec             138       0.45266\n",
      "15 2018-06-05 18:08:04  1 min 52.777 sec             173       0.45163\n",
      "16 2018-06-05 18:08:33  2 min 22.230 sec             221       0.45065\n",
      "17 2018-06-05 18:09:09  2 min 57.685 sec             276       0.45014\n",
      "18 2018-06-05 18:10:06  3 min 55.422 sec             364       0.44964\n",
      "19 2018-06-05 18:10:36  4 min 25.301 sec             400       0.44948\n",
      "   training_logloss training_auc training_lift training_classification_error\n",
      "1                                                                           \n",
      "2          14.07813      0.54962       1.16525                       0.65751\n",
      "3          12.44973      0.55721       1.20677                       0.65397\n",
      "4           5.45368      0.58882       1.35611                       0.65458\n",
      "5           2.28045      0.61571       1.51067                       0.51368\n",
      "6           1.28720      0.63150       1.78268                       0.51859\n",
      "7           0.97080      0.64205       1.78947                       0.49967\n",
      "8           0.80028      0.64943       1.88280                       0.48454\n",
      "9           0.69431      0.65778       1.95647                       0.46582\n",
      "10          0.64524      0.66564       1.94776                       0.45665\n",
      "11          0.61888      0.67182       1.96361                       0.42983\n",
      "12          0.61027      0.67589       2.02596                       0.43970\n",
      "13          0.60366      0.68023       2.07472                       0.43553\n",
      "14          0.59757      0.68485       2.05018                       0.41861\n",
      "15          0.59461      0.68865       2.09911                       0.42808\n",
      "16          0.59219      0.69214       2.17481                       0.42441\n",
      "17          0.59101      0.69417       2.19054                       0.40756\n",
      "18          0.58988      0.69609       2.18392                       0.40974\n",
      "19          0.58952      0.69678       2.18392                       0.40875\n",
      "   validation_rmse validation_logloss validation_auc validation_lift\n",
      "1                                                                   \n",
      "2          0.63552           13.88328        0.55703         1.20382\n",
      "3          0.55214            7.18595        0.58053         1.37656\n",
      "4          0.47859            1.15331        0.63063         1.86192\n",
      "5          0.46416            0.71379        0.65877         1.87386\n",
      "6          0.45900            0.63978        0.67215         2.02411\n",
      "7          0.45685            0.61545        0.67835         2.05757\n",
      "8          0.45520            0.60865        0.68295         2.07135\n",
      "9          0.45375            0.59886        0.68842         2.02431\n",
      "10         0.45267            0.59635        0.69231         1.94794\n",
      "11         0.45187            0.59465        0.69552         1.98585\n",
      "12         0.45163            0.59393        0.69675         2.13630\n",
      "13         0.45125            0.59319        0.69840         2.10743\n",
      "14         0.45047            0.59154        0.70192         2.10800\n",
      "15         0.45005            0.59073        0.70415         2.02082\n",
      "16         0.44989            0.59042        0.70461         2.02082\n",
      "17         0.44990            0.59058        0.70429         1.93422\n",
      "18         0.44955            0.58978        0.70596         1.96309\n",
      "19         0.44959            0.58982        0.70578         1.96309\n",
      "   validation_classification_error\n",
      "1                                 \n",
      "2                          0.64654\n",
      "3                          0.64654\n",
      "4                          0.50927\n",
      "5                          0.47764\n",
      "6                          0.45857\n",
      "7                          0.47815\n",
      "8                          0.41395\n",
      "9                          0.41220\n",
      "10                         0.40324\n",
      "11                         0.37881\n",
      "12                         0.39860\n",
      "13                         0.39571\n",
      "14                         0.39035\n",
      "15                         0.38819\n",
      "16                         0.39716\n",
      "17                         0.39056\n",
      "18                         0.38798\n",
      "19                         0.39077\n",
      "\n",
      "Variable Importances: (Extract with `h2o.varimp`) \n",
      "=================================================\n",
      "\n",
      "Variable Importances: \n",
      "      variable relative_importance scaled_importance percentage\n",
      "1        yrbrn       419080.875000          1.000000   0.083233\n",
      "2 social_trust       405522.343750          0.967647   0.080540\n",
      "3       eduyrs       387801.781250          0.925363   0.077021\n",
      "4    trust_exe       357294.625000          0.852567   0.070962\n",
      "5    trust_leg       344091.218750          0.821062   0.068340\n",
      "\n",
      "---\n",
      "             variable relative_importance scaled_importance percentage\n",
      "17 political_interest       166290.859375          0.396799   0.033027\n",
      "18             female        85124.312500          0.203121   0.016906\n",
      "19             round1        81464.671875          0.194389   0.016180\n",
      "20            married        78686.484375          0.187760   0.015628\n",
      "21           children        77571.015625          0.185098   0.015406\n",
      "22        houseperson        23861.841797          0.056939   0.004739\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>variable</th><th scope=col>relative_importance</th><th scope=col>scaled_importance</th><th scope=col>percentage</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>yrbrn             </td><td>419080.87500      </td><td>1.00000000000     </td><td>0.083233110189    </td></tr>\n",
       "\t<tr><td>social_trust      </td><td>405522.34375      </td><td>0.96764698162     </td><td>0.080540267845    </td></tr>\n",
       "\t<tr><td>eduyrs            </td><td>387801.78125      </td><td>0.92536263138     </td><td>0.077020809862    </td></tr>\n",
       "\t<tr><td>trust_exe         </td><td>357294.62500      </td><td>0.85256724015     </td><td>0.070961823043    </td></tr>\n",
       "\t<tr><td>trust_leg         </td><td>344091.21875      </td><td>0.82106161191     </td><td>0.068339511616    </td></tr>\n",
       "\t<tr><td>wkhtot            </td><td>327367.59375      </td><td>0.78115612828     </td><td>0.065018054100    </td></tr>\n",
       "\t<tr><td>trstep            </td><td>295335.81250      </td><td>0.70472271611     </td><td>0.058656263483    </td></tr>\n",
       "\t<tr><td>stfdem            </td><td>288184.96875      </td><td>0.68765955676     </td><td>0.057236043660    </td></tr>\n",
       "\t<tr><td>tvtot             </td><td>252775.23438      </td><td>0.60316575977     </td><td>0.050203362145    </td></tr>\n",
       "\t<tr><td>sclmeet           </td><td>245430.15625      </td><td>0.58563912336     </td><td>0.048744565685    </td></tr>\n",
       "\t<tr><td>church            </td><td>227783.35938      </td><td>0.54353079075     </td><td>0.045239758197    </td></tr>\n",
       "\t<tr><td>tvpol             </td><td>206055.67188      </td><td>0.49168474194     </td><td>0.040924450304    </td></tr>\n",
       "\t<tr><td>domicil           </td><td>203297.64062      </td><td>0.48510359874     </td><td>0.040376681287    </td></tr>\n",
       "\t<tr><td>tolerance         </td><td>194451.95312      </td><td>0.46399624685     </td><td>0.038619850741    </td></tr>\n",
       "\t<tr><td>self_realisation  </td><td>189057.25000      </td><td>0.45112354507     </td><td>0.037548415735    </td></tr>\n",
       "\t<tr><td>solidarity        </td><td>178496.34375      </td><td>0.42592338233     </td><td>0.035450927814    </td></tr>\n",
       "\t<tr><td>political_interest</td><td>166290.85938      </td><td>0.39679896959     </td><td>0.033026812359    </td></tr>\n",
       "\t<tr><td>female            </td><td> 85124.31250      </td><td>0.20312144404     </td><td>0.016906429534    </td></tr>\n",
       "\t<tr><td>round1            </td><td> 81464.67188      </td><td>0.19438890375     </td><td>0.016179593046    </td></tr>\n",
       "\t<tr><td>married           </td><td> 78686.48438      </td><td>0.18775966423     </td><td>0.015627820822    </td></tr>\n",
       "\t<tr><td>children          </td><td> 77571.01562      </td><td>0.18509796140     </td><td>0.015406279017    </td></tr>\n",
       "\t<tr><td>houseperson       </td><td> 23861.84180      </td><td>0.05693851288     </td><td>0.004739169516    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " variable & relative\\_importance & scaled\\_importance & percentage\\\\\n",
       "\\hline\n",
       "\t yrbrn              & 419080.87500       & 1.00000000000      & 0.083233110189    \\\\\n",
       "\t social\\_trust       & 405522.34375         & 0.96764698162        & 0.080540267845      \\\\\n",
       "\t eduyrs             & 387801.78125       & 0.92536263138      & 0.077020809862    \\\\\n",
       "\t trust\\_exe          & 357294.62500         & 0.85256724015        & 0.070961823043      \\\\\n",
       "\t trust\\_leg          & 344091.21875         & 0.82106161191        & 0.068339511616      \\\\\n",
       "\t wkhtot             & 327367.59375       & 0.78115612828      & 0.065018054100    \\\\\n",
       "\t trstep             & 295335.81250       & 0.70472271611      & 0.058656263483    \\\\\n",
       "\t stfdem             & 288184.96875       & 0.68765955676      & 0.057236043660    \\\\\n",
       "\t tvtot              & 252775.23438       & 0.60316575977      & 0.050203362145    \\\\\n",
       "\t sclmeet            & 245430.15625       & 0.58563912336      & 0.048744565685    \\\\\n",
       "\t church             & 227783.35938       & 0.54353079075      & 0.045239758197    \\\\\n",
       "\t tvpol              & 206055.67188       & 0.49168474194      & 0.040924450304    \\\\\n",
       "\t domicil            & 203297.64062       & 0.48510359874      & 0.040376681287    \\\\\n",
       "\t tolerance          & 194451.95312       & 0.46399624685      & 0.038619850741    \\\\\n",
       "\t self\\_realisation   & 189057.25000         & 0.45112354507        & 0.037548415735      \\\\\n",
       "\t solidarity         & 178496.34375       & 0.42592338233      & 0.035450927814    \\\\\n",
       "\t political\\_interest & 166290.85938         & 0.39679896959        & 0.033026812359      \\\\\n",
       "\t female             &  85124.31250       & 0.20312144404      & 0.016906429534    \\\\\n",
       "\t round1             &  81464.67188       & 0.19438890375      & 0.016179593046    \\\\\n",
       "\t married            &  78686.48438       & 0.18775966423      & 0.015627820822    \\\\\n",
       "\t children           &  77571.01562       & 0.18509796140      & 0.015406279017    \\\\\n",
       "\t houseperson        &  23861.84180       & 0.05693851288      & 0.004739169516    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "variable | relative_importance | scaled_importance | percentage | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| yrbrn              | 419080.87500       | 1.00000000000      | 0.083233110189     | \n",
       "| social_trust       | 405522.34375       | 0.96764698162      | 0.080540267845     | \n",
       "| eduyrs             | 387801.78125       | 0.92536263138      | 0.077020809862     | \n",
       "| trust_exe          | 357294.62500       | 0.85256724015      | 0.070961823043     | \n",
       "| trust_leg          | 344091.21875       | 0.82106161191      | 0.068339511616     | \n",
       "| wkhtot             | 327367.59375       | 0.78115612828      | 0.065018054100     | \n",
       "| trstep             | 295335.81250       | 0.70472271611      | 0.058656263483     | \n",
       "| stfdem             | 288184.96875       | 0.68765955676      | 0.057236043660     | \n",
       "| tvtot              | 252775.23438       | 0.60316575977      | 0.050203362145     | \n",
       "| sclmeet            | 245430.15625       | 0.58563912336      | 0.048744565685     | \n",
       "| church             | 227783.35938       | 0.54353079075      | 0.045239758197     | \n",
       "| tvpol              | 206055.67188       | 0.49168474194      | 0.040924450304     | \n",
       "| domicil            | 203297.64062       | 0.48510359874      | 0.040376681287     | \n",
       "| tolerance          | 194451.95312       | 0.46399624685      | 0.038619850741     | \n",
       "| self_realisation   | 189057.25000       | 0.45112354507      | 0.037548415735     | \n",
       "| solidarity         | 178496.34375       | 0.42592338233      | 0.035450927814     | \n",
       "| political_interest | 166290.85938       | 0.39679896959      | 0.033026812359     | \n",
       "| female             |  85124.31250       | 0.20312144404      | 0.016906429534     | \n",
       "| round1             |  81464.67188       | 0.19438890375      | 0.016179593046     | \n",
       "| married            |  78686.48438       | 0.18775966423      | 0.015627820822     | \n",
       "| children           |  77571.01562       | 0.18509796140      | 0.015406279017     | \n",
       "| houseperson        |  23861.84180       | 0.05693851288      | 0.004739169516     | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   variable           relative_importance scaled_importance percentage    \n",
       "1  yrbrn              419080.87500        1.00000000000     0.083233110189\n",
       "2  social_trust       405522.34375        0.96764698162     0.080540267845\n",
       "3  eduyrs             387801.78125        0.92536263138     0.077020809862\n",
       "4  trust_exe          357294.62500        0.85256724015     0.070961823043\n",
       "5  trust_leg          344091.21875        0.82106161191     0.068339511616\n",
       "6  wkhtot             327367.59375        0.78115612828     0.065018054100\n",
       "7  trstep             295335.81250        0.70472271611     0.058656263483\n",
       "8  stfdem             288184.96875        0.68765955676     0.057236043660\n",
       "9  tvtot              252775.23438        0.60316575977     0.050203362145\n",
       "10 sclmeet            245430.15625        0.58563912336     0.048744565685\n",
       "11 church             227783.35938        0.54353079075     0.045239758197\n",
       "12 tvpol              206055.67188        0.49168474194     0.040924450304\n",
       "13 domicil            203297.64062        0.48510359874     0.040376681287\n",
       "14 tolerance          194451.95312        0.46399624685     0.038619850741\n",
       "15 self_realisation   189057.25000        0.45112354507     0.037548415735\n",
       "16 solidarity         178496.34375        0.42592338233     0.035450927814\n",
       "17 political_interest 166290.85938        0.39679896959     0.033026812359\n",
       "18 female              85124.31250        0.20312144404     0.016906429534\n",
       "19 round1              81464.67188        0.19438890375     0.016179593046\n",
       "20 married             78686.48438        0.18775966423     0.015627820822\n",
       "21 children            77571.01562        0.18509796140     0.015406279017\n",
       "22 houseperson         23861.84180        0.05693851288     0.004739169516"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |======================================================================| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.60964550700742"
      ],
      "text/latex": [
       "0.60964550700742"
      ],
      "text/markdown": [
       "0.60964550700742"
      ],
      "text/plain": [
       "[1] 0.609645507"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.581990730523063"
      ],
      "text/latex": [
       "0.581990730523063"
      ],
      "text/markdown": [
       "0.581990730523063"
      ],
      "text/plain": [
       "[1] 0.5819907305"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |======================================================================| 100%\n",
      "Model Details:\n",
      "==============\n",
      "\n",
      "H2OBinomialModel: drf\n",
      "Model Key:  forestbal \n",
      "Model Summary: \n",
      "  number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n",
      "1             400                      400            96897523        30\n",
      "  max_depth mean_depth min_leaves max_leaves mean_leaves\n",
      "1        30   30.00000      18947      19598 19301.04700\n",
      "\n",
      "H2OBinomialMetrics: drf\n",
      "** Reported on training data. **\n",
      "** Metrics reported on Out-Of-Bag training samples **\n",
      "\n",
      "MSE:  0.09852366327\n",
      "RMSE:  0.313884793\n",
      "LogLoss:  0.3319074294\n",
      "Mean Per-Class Error:  0.08005145978\n",
      "AUC:  0.9538597217\n",
      "Gini:  0.9077194434\n",
      "\n",
      "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
      "           0     1    Error          Rate\n",
      "0      55984  1148 0.020094   =1148/57132\n",
      "1       7999 49133 0.140009   =7999/57132\n",
      "Totals 63983 50281 0.080051  =9147/114264\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "                        metric threshold    value idx\n",
      "1                       max f1  0.642835 0.914843 158\n",
      "2                       max f2  0.429102 0.896771 234\n",
      "3                 max f0point5  0.674598 0.955539 149\n",
      "4                 max accuracy  0.642835 0.919949 158\n",
      "5                max precision  0.999903 1.000000   0\n",
      "6                   max recall  0.054134 1.000000 394\n",
      "7              max specificity  0.999903 1.000000   0\n",
      "8             max absolute_mcc  0.658251 0.846442 154\n",
      "9   max min_per_class_accuracy  0.517928 0.893422 200\n",
      "10 max mean_per_class_accuracy  0.642835 0.919949 158\n",
      "\n",
      "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
      "H2OBinomialMetrics: drf\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE:  0.2020843855\n",
      "RMSE:  0.4495379689\n",
      "LogLoss:  0.589613789\n",
      "Mean Per-Class Error:  0.3572129321\n",
      "AUC:  0.7061921057\n",
      "Gini:  0.4123842114\n",
      "\n",
      "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
      "          0    1    Error        Rate\n",
      "0      3116 3158 0.503347  =3158/6274\n",
      "1       724 2706 0.211079   =724/3430\n",
      "Totals 3840 5864 0.400041  =3882/9704\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "                        metric threshold    value idx\n",
      "1                       max f1  0.314455 0.582311 251\n",
      "2                       max f2  0.180711 0.742090 344\n",
      "3                 max f0point5  0.445496 0.544040 156\n",
      "4                 max accuracy  0.487943 0.687036 127\n",
      "5                max precision  0.860513 1.000000   0\n",
      "6                   max recall  0.061281 1.000000 395\n",
      "7              max specificity  0.860513 1.000000   0\n",
      "8             max absolute_mcc  0.373359 0.297034 208\n",
      "9   max min_per_class_accuracy  0.373359 0.652770 208\n",
      "10 max mean_per_class_accuracy  0.372056 0.654759 209\n",
      "\n",
      "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
      "\n",
      "\n",
      "Scoring History: \n",
      "            timestamp   duration number_of_trees training_rmse training_logloss\n",
      "1 2018-06-05 18:10:59  0.016 sec               0                               \n",
      "2 2018-06-05 18:11:01  1.353 sec               1       0.50176          8.61235\n",
      "3 2018-06-05 18:11:01  2.182 sec               2       0.48375          7.47197\n",
      "4 2018-06-05 18:11:02  2.972 sec               3       0.46661          6.36888\n",
      "5 2018-06-05 18:11:03  3.951 sec               4       0.45124          5.44934\n",
      "  training_auc training_lift training_classification_error validation_rmse\n",
      "1                                                                         \n",
      "2      0.74772       1.42889                       0.25293         0.64581\n",
      "3      0.76716       1.47789                       0.24081         0.55350\n",
      "4      0.78482       1.52443                       0.23162         0.52007\n",
      "5      0.80095       1.57273                       0.22417         0.50396\n",
      "  validation_logloss validation_auc validation_lift\n",
      "1                                                  \n",
      "2           14.30953        0.54875         1.16536\n",
      "3            7.12193        0.58080         1.36225\n",
      "4            4.02012        0.59563         1.49805\n",
      "5            2.61354        0.60452         1.59080\n",
      "  validation_classification_error\n",
      "1                                \n",
      "2                         0.64654\n",
      "3                         0.64654\n",
      "4                         0.50938\n",
      "5                         0.54081\n",
      "\n",
      "---\n",
      "             timestamp          duration number_of_trees training_rmse\n",
      "17 2018-06-05 18:12:49  1 min 50.012 sec             139       0.31703\n",
      "18 2018-06-05 18:13:15  2 min 16.125 sec             172       0.31612\n",
      "19 2018-06-05 18:13:50  2 min 50.403 sec             217       0.31532\n",
      "20 2018-06-05 18:14:37  3 min 37.913 sec             275       0.31477\n",
      "21 2018-06-05 18:15:41  4 min 41.958 sec             346       0.31421\n",
      "22 2018-06-05 18:16:37  5 min 38.059 sec             400       0.31388\n",
      "   training_logloss training_auc training_lift training_classification_error\n",
      "17          0.33497      0.95134       2.00000                       0.08620\n",
      "18          0.33405      0.95204       2.00000                       0.08461\n",
      "19          0.33326      0.95275       2.00000                       0.08298\n",
      "20          0.33279      0.95323       2.00000                       0.08167\n",
      "21          0.33222      0.95362       2.00000                       0.08102\n",
      "22          0.33191      0.95386       2.00000                       0.08005\n",
      "   validation_rmse validation_logloss validation_auc validation_lift\n",
      "17         0.45048            0.59152        0.70203         1.87648\n",
      "18         0.45000            0.59057        0.70391         1.99196\n",
      "19         0.44987            0.59029        0.70471         1.90535\n",
      "20         0.44966            0.58988        0.70563         1.93422\n",
      "21         0.44972            0.59002        0.70544         1.90535\n",
      "22         0.44954            0.58961        0.70619         1.96309\n",
      "   validation_classification_error\n",
      "17                         0.40210\n",
      "18                         0.38613\n",
      "19                         0.41323\n",
      "20                         0.41066\n",
      "21                         0.38407\n",
      "22                         0.40004\n",
      "\n",
      "Variable Importances: (Extract with `h2o.varimp`) \n",
      "=================================================\n",
      "\n",
      "Variable Importances: \n",
      "      variable relative_importance scaled_importance percentage\n",
      "1        yrbrn       604090.312500          1.000000   0.082815\n",
      "2 social_trust       599738.500000          0.992796   0.082218\n",
      "3       eduyrs       573276.937500          0.948992   0.078591\n",
      "4    trust_exe       521383.343750          0.863088   0.071476\n",
      "5    trust_leg       499993.406250          0.827680   0.068544\n",
      "\n",
      "---\n",
      "             variable relative_importance scaled_importance percentage\n",
      "17 political_interest       247159.171875          0.409143   0.033883\n",
      "18             female       119272.125000          0.197441   0.016351\n",
      "19             round1       115454.625000          0.191121   0.015828\n",
      "20            married       111620.742188          0.184775   0.015302\n",
      "21           children       109130.898438          0.180653   0.014961\n",
      "22        houseperson        32082.544922          0.053109   0.004398\n",
      "  |======================================================================| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.600164880461665"
      ],
      "text/latex": [
       "0.600164880461665"
      ],
      "text/markdown": [
       "0.600164880461665"
      ],
      "text/plain": [
       "[1] 0.6001648805"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.581806423798232"
      ],
      "text/latex": [
       "0.581806423798232"
      ],
      "text/markdown": [
       "0.581806423798232"
      ],
      "text/plain": [
       "[1] 0.5818064238"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |======================================================================| 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "    true\n",
       "pred    0    1\n",
       "   0 3279  793\n",
       "   1 2995 2637"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n"
     ]
    }
   ],
   "source": [
    "#m2a <- h2o.deeplearning(\n",
    "#  x = xnames,\n",
    "#  #y = \"Outcome\",\n",
    "#  training_frame= h2o.train,\n",
    "#  validation_frame = h2o.test,\n",
    "#  activation = \"Tanh\",\n",
    "#  autoencoder = TRUE,\n",
    "#  hidden = c(20),\n",
    "#  epochs = 10,\n",
    "#  sparsity_beta = 0,\n",
    "#  l1 = 0,\n",
    "#  l2 = 0\n",
    "#)\n",
    "\n",
    "#m2b <- h2o.deeplearning(\n",
    "#  x = xnames,\n",
    "#  #y = \"Outcome\",\n",
    "#  training_frame= h2o.train,\n",
    "#  validation_frame = h2o.test,\n",
    "#  activation = \"Tanh\",\n",
    "#  autoencoder = TRUE,\n",
    "#  hidden = c(20, 15),\n",
    "#  epochs = 10,\n",
    "#  sparsity_beta = 0,\n",
    "#  #hidden_pout_ratios = c(.3),\n",
    "#  l1 = 0,\n",
    "#  l2 = 0\n",
    "#)\n",
    "\n",
    "#m2c <- h2o.deeplearning(\n",
    "#  x = xnames,\n",
    "#  #y = \"Outcome\",\n",
    "#  training_frame= h2o.train,\n",
    "#  validation_frame = h2o.test,\n",
    "#  activation = \"Tanh\",\n",
    "#  autoencoder = TRUE,\n",
    "#  hidden = c(20, 15, 10),\n",
    "#  epochs = 10,\n",
    "#  sparsity_beta = 0,\n",
    "#  l1 = 0,\n",
    "#  l2 = 0\n",
    "#)\n",
    "\n",
    "#m2d <- h2o.deeplearning(\n",
    "#  x = xnames,\n",
    "#  #y = \"Outcome\",\n",
    "#  training_frame= h2o.train,\n",
    "#  validation_frame = h2o.test,\n",
    "#  activation = \"Tanh\",\n",
    "#  autoencoder = TRUE,\n",
    "#  hidden = c(20, 15, 10, 5),\n",
    "#  epochs = 10,\n",
    "#  sparsity_beta = 0,\n",
    "#  #hi2den_dropout_ratios = c(.3),\n",
    "#  l1 = 0,\n",
    "#  l2 = 0\n",
    "#)\n",
    "\n",
    "#summary(m2a)\n",
    "#summary(m2b)\n",
    "#summary(m2c)\n",
    "#summary(m2d)\n",
    "\n",
    "#error1 <- as.data.frame(h2o.anomaly(m2a, h2o.train))\n",
    "#error2a <- as.data.frame(h2o.anomaly(m2b, h2o.train))\n",
    "#error2b <- as.data.frame(h2o.anomaly(m2c, h2o.train))\n",
    "#error2c <- as.data.frame(h2o.anomaly(m2d, h2o.train))\n",
    "\n",
    "#error <- as.data.table(rbind(\n",
    "#  cbind.data.frame(Model = \"2a\", error1),\n",
    "#  cbind.data.frame(Model = \"2b\", error2a),\n",
    "#  cbind.data.frame(Model = \"2c\", error2b),\n",
    "#  cbind.data.frame(Model = \"2d\", error2c)))\n",
    "\n",
    "#percentile <- error[, .(\n",
    "#  Percentile = quantile(Reconstruction.MSE, probs = .95)\n",
    "#), by = Model]\n",
    "\n",
    "#p1 <- ggplot(error, aes(Reconstruction.MSE)) +\n",
    "#  geom_histogram(binwidth = .001, fill = \"grey50\") +\n",
    "#  geom_vline(aes(xintercept = Percentile), data = percentile, linetype = 2) +\n",
    "#  theme_bw() +\n",
    "#  facet_wrap(~Model)\n",
    "#print(p1)\n",
    "\n",
    "#png(\"error_1.png\",\n",
    "#    width = 5.5, height = 5.5, units = \"in\", res = 600)\n",
    "#print(p1)\n",
    "#dev.off()\n",
    "\n",
    "#in sample = 0.70\n",
    "mt3 <- h2o.deeplearning(\n",
    "  x = xnames,\n",
    "  y = \"volact\",\n",
    "  training_frame = h2o.train,\n",
    "  validation_frame = h2o.test,\n",
    "  activation = \"Rectifier\",\n",
    "  hidden = c(200, 200, 100),\n",
    "  epochs = 10,\n",
    "  rate = .005,\n",
    "  loss = \"CrossEntropy\",\n",
    "  #input_dropout_ratio = .2,\n",
    "  #hidden_dropout_ratios = c(.5, .3, .1),\n",
    "  export_weights_and_biases = TRUE\n",
    ")\n",
    "summary(mt3)\n",
    "\n",
    "mt3_pred <- h2o.predict(mt3, h2o.test)\n",
    "ACC(table(as.vector(mt3_pred$predict), as.vector(h2o.test$volact)))\n",
    "F1(table(as.vector(mt3_pred$predict), as.vector(h2o.test$volact)))\n",
    "\n",
    "h2o.varimp(mt3)\n",
    "\n",
    "#balanced training set\n",
    "mtbal <- h2o.deeplearning(\n",
    "  x = xnames,\n",
    "  y = \"volact\",\n",
    "  training_frame = h2o.train.bal,\n",
    "  validation_frame = h2o.test,\n",
    "  activation = \"Rectifier\",\n",
    "  hidden = c(200, 200, 100),\n",
    "  epochs = 10,\n",
    "  rate = .005,\n",
    "  loss = \"CrossEntropy\",\n",
    "  #input_dropout_ratio = .2,\n",
    "  #hidden_dropout_ratios = c(.5, .3, .1),\n",
    "  export_weights_and_biases = TRUE\n",
    ")\n",
    "summary(mtbal)\n",
    "\n",
    "mtbal_pred <- h2o.predict(mtbal, h2o.test)\n",
    "ACC(table(as.vector(mtbal_pred$predict), as.vector(h2o.test$volact)))\n",
    "F1(table(as.vector(mtbal_pred$predict), as.vector(h2o.test$volact)))\n",
    "\n",
    "#h2o.varimp(mtbal)\n",
    "\n",
    "## weights for mapping from inputs to hidden layer 1 neurons\n",
    "w1 <- as.matrix(h2o.weights(mtbal, 1))\n",
    "\n",
    "## plot heatmap of the weights\n",
    "tmp <- as.data.frame(t(w1))\n",
    "tmp$Row <- 1:nrow(tmp)\n",
    "tmp <- melt(tmp, id.vars = c(\"Row\"))\n",
    "\n",
    "#p.heat <- ggplot(tmp,\n",
    "#       aes(variable, Row, fill = value)) +\n",
    "#  geom_tile() +\n",
    "#  scale_fill_gradientn(colours = c(\"black\", \"white\", \"blue\")) +\n",
    "#  theme_classic() +\n",
    "#  theme(axis.text = element_blank()) +\n",
    "#  xlab(\"Hidden Neuron\") +\n",
    "#  ylab(\"Input Variable\") +\n",
    "#  ggtitle(\"Heatmap of Weights for Layer 1\")\n",
    "#print(p.heat)\n",
    "\n",
    "#png(\"heatmap_layer1.png\",\n",
    "#    width = 5.5, height = 7.5, units = \"in\", res = 600)\n",
    "#print(p.heat)\n",
    "#dev.off()\n",
    "\n",
    "rf2 <- h2o.randomForest(        \n",
    "  training_frame = h2o.train,        \n",
    "  validation_frame = h2o.test,      \n",
    "  x = xnames,                        \n",
    "  y = \"volact\",                          \n",
    "  model_id = \"forest2\",    \n",
    "  ntrees = 400,            \n",
    "  max_depth = 30,\n",
    "  stopping_rounds = 0,           \n",
    "  score_each_iteration = F,     \n",
    "  seed = 1000000) \n",
    "summary(rf2)\n",
    "\n",
    "h2o.varimp(rf2)\n",
    "\n",
    "rf2_pred <- h2o.predict(rf2, h2o.test)\n",
    "ACC(table(as.vector(rf2_pred$predict), as.vector(h2o.test$volact)))\n",
    "F1(table(as.vector(rf2_pred$predict), as.vector(h2o.test$volact)))\n",
    "\n",
    "rfbal <- h2o.randomForest(        \n",
    "  training_frame = h2o.train.bal,        \n",
    "  validation_frame = h2o.test,      \n",
    "  x = xnames,                        \n",
    "  y = \"volact\",                          \n",
    "  model_id = \"forestbal\",    \n",
    "  ntrees = 400,            \n",
    "  max_depth = 30,\n",
    "  stopping_rounds = 0,           \n",
    "  score_each_iteration = F,     \n",
    "  seed = 1000000) \n",
    "summary(rfbal)\n",
    "\n",
    "rfbal_pred <- h2o.predict(rfbal, h2o.test)\n",
    "ACC(table(as.vector(rfbal_pred$predict), as.vector(h2o.test$volact)))\n",
    "F1(table(as.vector(rfbal_pred$predict), as.vector(h2o.test$volact)))\n",
    "\n",
    "rf2_pred <- h2o.predict(rf2, h2o.test)\n",
    "table(\"pred\"=as.vector(rf2_pred$predict), \"true\"=as.vector(h2o.test$volact))\n",
    "\n",
    "#png(\"corrplot.png\", width = 5.5, height = 5.5, units = \"in\", res = 600)\n",
    "#c <- corrplot(res, type = \"upper\", order = \"hclust\", tl.col = \"black\", tl.srt = 45)\n",
    "#dev.off()\n",
    "#print(c)\n",
    "\n",
    "range <- seq(-4, 2, length.out = 20)\n",
    "sel <- 7\n",
    "plot.data <- matrix(NA, nrow = length(range), ncol = ncol(test[,-1]))\n",
    "for(i in 2:ncol(test)){\n",
    "    plot.data[,i-1] <- mean(test[,i])\n",
    "}\n",
    "for(i in 1:length(range)){\n",
    "    plot.data[i, sel] <- range[i]\n",
    "}\n",
    "colnames(plot.data) <- xnames\n",
    "\n",
    "h2o.plot <- as.h2o(plot.data)\n",
    "\n",
    "mt3_pred <- h2o.predict(mt3, h2o.plot)\n",
    "\n",
    "#df_nn <- data.frame(cbind(range, as.vector(mt3_pred$p1)))\n",
    "\n",
    "#png(\"nn-age-volact.png\")\n",
    "#myplot <- ggplot(df_nn,aes(x = df_nn[,1], y = df_nn[,2], ymin = 0, ymax = 0)) + \n",
    "#    geom_pointrange(col = \"blue\") + \n",
    "#    ylab(\"Predicted Probability of Voluntary Action\") + \n",
    "#    xlab(\"Year of Birth (Scaled)\") + \n",
    "#    ggtitle(\"Neural Net: \\n Predicted Probabilites of Voluntary Action for a range of Year of Births\") + \n",
    "#    scale_x_continuous(breaks = range) + \n",
    "#    theme_bw() \n",
    "#print(myplot)\n",
    "#dev.off()\n",
    "#print(myplot)\n",
    "\n",
    "rf2_pred <- h2o.predict(rf2, h2o.plot)\n",
    "\n",
    "#df_rf <- data.frame(cbind(range, as.vector(rf2_pred$p1)))\n",
    "\n",
    "#png(\"rf-age-volact.png\")\n",
    "#myplot <- ggplot(df_rf,aes(x = df_rf[,1], y = df_rf[,2], ymin = 0, ymax = 0)) + \n",
    "#    geom_pointrange(col = \"green\") + \n",
    "#    ylab(\"Predicted Probability of Voluntary Action\") + \n",
    "#    xlab(\"Year of Birth (Scaled)\") + \n",
    "#    ggtitle(\"Random Forest: \\n Predicted Probabilites of Voluntary Action for a range of Year of Births\") + \n",
    "#    scale_x_continuous(breaks = range) + \n",
    "#    theme_bw() \n",
    "#print(myplot)\n",
    "#dev.off()\n",
    "#print(myplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "age.volact <- data.frame(cbind(as.matrix(age.volact), as.vector(mt3_pred$p1), as.vector(rf2_pred$p1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames(age.volact) <- c(\"range\", \"1\", \"2\", \"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>range</th><th scope=col>logistic regression</th><th scope=col>neural net</th><th scope=col>random forest</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>-4.000000000</td><td>0.2435812039</td><td>0.1731957564</td><td>0.1283333333</td></tr>\n",
       "\t<tr><td>-3.684210526</td><td>0.2606669144</td><td>0.1852403339</td><td>0.1283333333</td></tr>\n",
       "\t<tr><td>-3.368421053</td><td>0.2785098154</td><td>0.1980305383</td><td>0.1283333333</td></tr>\n",
       "\t<tr><td>-3.052631579</td><td>0.2970833043</td><td>0.2117560907</td><td>0.1283333333</td></tr>\n",
       "\t<tr><td>-2.736842105</td><td>0.3163523281</td><td>0.2271521981</td><td>0.1258333333</td></tr>\n",
       "\t<tr><td>-2.421052632</td><td>0.3362732523</td><td>0.2435791395</td><td>0.1233333333</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " range & logistic regression & neural net & random forest\\\\\n",
       "\\hline\n",
       "\t -4.000000000 & 0.2435812039 & 0.1731957564 & 0.1283333333\\\\\n",
       "\t -3.684210526 & 0.2606669144 & 0.1852403339 & 0.1283333333\\\\\n",
       "\t -3.368421053 & 0.2785098154 & 0.1980305383 & 0.1283333333\\\\\n",
       "\t -3.052631579 & 0.2970833043 & 0.2117560907 & 0.1283333333\\\\\n",
       "\t -2.736842105 & 0.3163523281 & 0.2271521981 & 0.1258333333\\\\\n",
       "\t -2.421052632 & 0.3362732523 & 0.2435791395 & 0.1233333333\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "range | logistic regression | neural net | random forest | \n",
       "|---|---|---|---|---|---|\n",
       "| -4.000000000 | 0.2435812039 | 0.1731957564 | 0.1283333333 | \n",
       "| -3.684210526 | 0.2606669144 | 0.1852403339 | 0.1283333333 | \n",
       "| -3.368421053 | 0.2785098154 | 0.1980305383 | 0.1283333333 | \n",
       "| -3.052631579 | 0.2970833043 | 0.2117560907 | 0.1283333333 | \n",
       "| -2.736842105 | 0.3163523281 | 0.2271521981 | 0.1258333333 | \n",
       "| -2.421052632 | 0.3362732523 | 0.2435791395 | 0.1233333333 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  range        logistic regression neural net   random forest\n",
       "1 -4.000000000 0.2435812039        0.1731957564 0.1283333333 \n",
       "2 -3.684210526 0.2606669144        0.1852403339 0.1283333333 \n",
       "3 -3.368421053 0.2785098154        0.1980305383 0.1283333333 \n",
       "4 -3.052631579 0.2970833043        0.2117560907 0.1283333333 \n",
       "5 -2.736842105 0.3163523281        0.2271521981 0.1258333333 \n",
       "6 -2.421052632 0.3362732523        0.2435791395 0.1233333333 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(age.volact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted <- melt(age.volact, id.vars=\"range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>range</th><th scope=col>variable</th><th scope=col>value</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>-4.000000000</td><td>1           </td><td>0.2435812039</td></tr>\n",
       "\t<tr><td>-3.684210526</td><td>1           </td><td>0.2606669144</td></tr>\n",
       "\t<tr><td>-3.368421053</td><td>1           </td><td>0.2785098154</td></tr>\n",
       "\t<tr><td>-3.052631579</td><td>1           </td><td>0.2970833043</td></tr>\n",
       "\t<tr><td>-2.736842105</td><td>1           </td><td>0.3163523281</td></tr>\n",
       "\t<tr><td>-2.421052632</td><td>1           </td><td>0.3362732523</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " range & variable & value\\\\\n",
       "\\hline\n",
       "\t -4.000000000 & 1            & 0.2435812039\\\\\n",
       "\t -3.684210526 & 1            & 0.2606669144\\\\\n",
       "\t -3.368421053 & 1            & 0.2785098154\\\\\n",
       "\t -3.052631579 & 1            & 0.2970833043\\\\\n",
       "\t -2.736842105 & 1            & 0.3163523281\\\\\n",
       "\t -2.421052632 & 1            & 0.3362732523\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "range | variable | value | \n",
       "|---|---|---|---|---|---|\n",
       "| -4.000000000 | 1            | 0.2435812039 | \n",
       "| -3.684210526 | 1            | 0.2606669144 | \n",
       "| -3.368421053 | 1            | 0.2785098154 | \n",
       "| -3.052631579 | 1            | 0.2970833043 | \n",
       "| -2.736842105 | 1            | 0.3163523281 | \n",
       "| -2.421052632 | 1            | 0.3362732523 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  range        variable value       \n",
       "1 -4.000000000 1        0.2435812039\n",
       "2 -3.684210526 1        0.2606669144\n",
       "3 -3.368421053 1        0.2785098154\n",
       "4 -3.052631579 1        0.2970833043\n",
       "5 -2.736842105 1        0.3163523281\n",
       "6 -2.421052632 1        0.3362732523"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(melted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#png(\"age-volact.png\")\n",
    "#myplot <- ggplot(data=melted, aes(x=range, y=value, group=variable)) + geom_line(col = melted$variable) +\n",
    "#    ylab(\"Predicted Probability of Voluntary Action\") + \n",
    "#    xlab(\"Year of Birth (Scaled)\") + \n",
    "#    ggtitle(\"Comparison: \\n Predicted Probabilites of Voluntary Action for a range of Year of Births \\n From top to down: Logistic Regression, Neural Network, Random Forest\") + \n",
    "#    theme_bw() \n",
    "#print(myplot)\n",
    "#dev.off()\n",
    "#print(myplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess1 <- read.table(\"C:/Users/laris/Desktop/MMDS/Semester3-FSS2018/Advanced-Quantitative-Methods/paper/ess1_with-vars-nolabel.csv\", header=TRUE, sep=\",\") \n",
    "\n",
    "ess6 <- read.table(\"C:/Users/laris/Desktop/MMDS/Semester3-FSS2018/Advanced-Quantitative-Methods/paper/ess6_with-vars-nolabel.csv\", header=TRUE, sep=\",\") \n",
    "\n",
    "for(i in c(4,5,6,10,11,12,13,14,15,16,17,18,19,20,21,22,23)){\n",
    "    ess1[is.na(ess1[,i]), i] <- mean(ess1[,i], na.rm = TRUE)\n",
    "}\n",
    "\n",
    "ess1[is.na(ess1[,3]), 3] <- 1\n",
    "ess1[is.na(ess1[,7]), 7] <- 1\n",
    "ess1[is.na(ess1[,8]), 8] <- 0\n",
    "ess1[is.na(ess1[,9]), 9] <- 0\n",
    "\n",
    "for(i in c(4,5,6,10,11,12,13,14,15,16,17,18,19,20,21,22,23)){\n",
    "    ess6[is.na(ess6[,i]), i] <- mean(ess6[,i], na.rm = TRUE)\n",
    "}\n",
    "\n",
    "ess6[is.na(ess6[,3]), 3] <- 1\n",
    "ess6[is.na(ess6[,7]), 7] <- 1\n",
    "ess6[is.na(ess6[,8]), 8] <- 0\n",
    "ess6[is.na(ess6[,9]), 9] <- 0\n",
    "\n",
    "ess1.scaled <- cbind(ess1[c(1, 3, 7, 8, 9)], scale(ess1[-c(1, 2, 3, 7, 8, 9, 24)]))\n",
    "\n",
    "ess1c.scaled <- cbind(ess1[c(1, 2, 3, 7, 8, 9)], scale(ess1[-c(1, 2, 3, 7, 8, 9, 24)]))\n",
    "\n",
    "ess6.scaled <- cbind(ess6[c(1, 3, 7, 8, 9)], scale(ess6[-c(1, 2, 3, 7, 8, 9, 24)]))\n",
    "\n",
    "ess6c.scaled <- cbind(ess6[c(1, 2, 3, 7, 8, 9)], scale(ess6[-c(1, 2, 3, 7, 8, 9, 24)]))\n",
    "\n",
    "for(t in unique(ess1c.scaled$cntry)) {\n",
    "    ess1c.scaled[paste(\"cntry\",t,sep=\"\")] <- ifelse(ess1c.scaled$cntry==t,1,0)\n",
    "}\n",
    "\n",
    "ess1c.scaled <- ess1c.scaled[,-2]\n",
    "\n",
    "for(t in unique(ess6c.scaled$cntry)) {\n",
    "    ess6c.scaled[paste(\"cntry\",t,sep=\"\")] <- ifelse(ess6c.scaled$cntry==t,1,0)\n",
    "}\n",
    "\n",
    "ess6c.scaled <- ess6c.scaled[,-2]\n",
    "\n",
    "#write.csv(ess1.scaled, file = \"ess1-scaled.csv\")\n",
    "#write.csv(ess1c.scaled, file = \"ess1-scaled-cntry.csv\")\n",
    "\n",
    "#write.csv(ess6.scaled, file = \"ess6-scaled.csv\")\n",
    "#write.csv(ess6c.scaled, file = \"ess6-scaled-cntry.csv\")\n",
    "\n",
    "#ess1.scaled <- read.table(\"ess1-scaled.csv\", header=TRUE, sep=\",\") \n",
    "#ess6.scaled <- read.table(\"ess6-scaled.csv\", header=TRUE, sep=\",\") \n",
    "#ess1c.scaled <- read.table(\"ess1-scaled-cntry.csv\", header=TRUE, sep=\",\") \n",
    "#ess6c.scaled <- read.table(\"ess6-scaled-cntry.csv\", header=TRUE, sep=\",\") \n",
    "\n",
    "#ess1.scaled <- ess1.scaled[,-1]\n",
    "#ess6.scaled <- ess6.scaled[,-1]\n",
    "\n",
    "#ess1c.scaled <- ess1c.scaled[,-1]\n",
    "#ess6c.scaled <- ess6c.scaled[,-1]\n",
    "\n",
    "set.seed(123)\n",
    "smp_size <- floor(0.9 * nrow(ess1.scaled))\n",
    "train_ind <- sample(seq_len(nrow(ess1.scaled)), size = smp_size)\n",
    "train1 <- ess1.scaled[train_ind,]\n",
    "test1 <- ess1.scaled[-train_ind,]\n",
    "train1c <- ess1c.scaled[train_ind,]\n",
    "test1c <- ess1c.scaled[-train_ind,]\n",
    "\n",
    "set.seed(123)\n",
    "smp_size <- floor(0.9 * nrow(ess6.scaled))\n",
    "train_ind <- sample(seq_len(nrow(ess6.scaled)), size = smp_size)\n",
    "train6 <- ess6.scaled[train_ind,]\n",
    "test6 <- ess6.scaled[-train_ind,]\n",
    "train6c <- ess6c.scaled[train_ind,]\n",
    "test6c <- ess6c.scaled[-train_ind,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 <- train1$volact\n",
    "X1 <- as.matrix(train1[,-1])\n",
    "ols_model1 <- glm(y1 ~ X1, family=binomial(link='logit'))\n",
    "X1 <- as.matrix(test1[,-1])\n",
    "p_ols1 <- predict(ols_model1, data.frame(X1), type = \"response\")\n",
    "\n",
    "#table(p_ols1, test1$volact)\n",
    "#ACC(table(p_ols1, test1$volact))\n",
    "#F1(table(p_ols1, test1$volact))\n",
    "\n",
    "y6 <- train6$volact\n",
    "X6 <- as.matrix(train6[,-1])\n",
    "ols_model6 <- glm(y6 ~ X6, family=binomial(link='logit'))\n",
    "X6 <- as.matrix(test6[,-1])\n",
    "p_ols6 <- predict(ols_model6, data.frame(X6), type = \"response\")\n",
    "\n",
    "#table(y_hat6, test6$volact)\n",
    "#ACC(table(y_hat6, test6$volact))\n",
    "#F1(table(y_hat6, test6$volact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.683899905571294"
      ],
      "text/latex": [
       "0.683899905571294"
      ],
      "text/markdown": [
       "0.683899905571294"
      ],
      "text/plain": [
       "[1] 0.6838999056"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.547115635604281"
      ],
      "text/latex": [
       "0.547115635604281"
      ],
      "text/markdown": [
       "0.547115635604281"
      ],
      "text/plain": [
       "[1] 0.5471156356"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_ols11 <- ifelse(p_ols1 > 0.1, 1, 0)\n",
    "p_ols12 <- ifelse(p_ols1 > 0.2, 1, 0)\n",
    "p_ols13 <- ifelse(p_ols1 > 0.3, 1, 0)\n",
    "p_ols14 <- ifelse(p_ols1 > 0.4, 1, 0)\n",
    "p_ols15 <- ifelse(p_ols1 > 0.5, 1, 0)\n",
    "p_ols16 <- ifelse(p_ols1 > 0.6, 1, 0)\n",
    "p_ols17 <- ifelse(p_ols1 > 0.7, 1, 0)\n",
    "#p_ols18 <- ifelse(p_ols1 > 0.8, 1, 0)\n",
    "#p_ols19 <- ifelse(p_ols1 > 0.9, 1, 0)\n",
    "\n",
    "threshold <- cbind(seq(0.1, 0.7, length.out = 7),\n",
    "    \n",
    "c(ACC(table(p_ols11, test1$volact)),\n",
    "ACC(table(p_ols12, test1$volact)),\n",
    "ACC(table(p_ols13, test1$volact)),\n",
    "ACC(table(p_ols14, test1$volact)),\n",
    "ACC(table(p_ols15, test1$volact)),\n",
    "ACC(table(p_ols16, test1$volact)),\n",
    "ACC(table(p_ols17, test1$volact))),\n",
    "#ACC(table(p_ols18, test1$volact)),\n",
    "#ACC(table(p_ols19, test1$volact))),\n",
    "\n",
    "c(F1(table(p_ols11, test1$volact)),\n",
    "F1(table(p_ols12, test1$volact)),\n",
    "F1(table(p_ols13, test1$volact)),\n",
    "F1(table(p_ols14, test1$volact)),\n",
    "F1(table(p_ols15, test1$volact)),\n",
    "F1(table(p_ols16, test1$volact)),\n",
    "F1(table(p_ols17, test1$volact))))\n",
    "#F1(table(p_ols18, test1$volact))))\n",
    "#F1(table(p_ols19, test1$volact))))\n",
    "\n",
    "max(threshold[,2])\n",
    "max(threshold[,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.68050475493782"
      ],
      "text/latex": [
       "0.68050475493782"
      ],
      "text/markdown": [
       "0.68050475493782"
      ],
      "text/plain": [
       "[1] 0.6805047549"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.584905660377358"
      ],
      "text/latex": [
       "0.584905660377358"
      ],
      "text/markdown": [
       "0.584905660377358"
      ],
      "text/plain": [
       "[1] 0.5849056604"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_ols61 <- ifelse(p_ols6 > 0.1, 1, 0)\n",
    "p_ols62 <- ifelse(p_ols6 > 0.2, 1, 0)\n",
    "p_ols63 <- ifelse(p_ols6 > 0.3, 1, 0)\n",
    "p_ols64 <- ifelse(p_ols6 > 0.4, 1, 0)\n",
    "p_ols65 <- ifelse(p_ols6 > 0.5, 1, 0)\n",
    "p_ols66 <- ifelse(p_ols6 > 0.6, 1, 0)\n",
    "p_ols67 <- ifelse(p_ols6 > 0.7, 1, 0)\n",
    "p_ols68 <- ifelse(p_ols6 > 0.8, 1, 0)\n",
    "#p_ols69 <- ifelse(p_ols6 > 0.9, 1, 0)\n",
    "\n",
    "threshold <- cbind(seq(0.1, 0.8, length.out = 8),\n",
    "    \n",
    "c(ACC(table(p_ols61, test6$volact)),\n",
    "ACC(table(p_ols62, test6$volact)),\n",
    "ACC(table(p_ols63, test6$volact)),\n",
    "ACC(table(p_ols64, test6$volact)),\n",
    "ACC(table(p_ols65, test6$volact)),\n",
    "ACC(table(p_ols66, test6$volact)),\n",
    "ACC(table(p_ols67, test6$volact)),\n",
    "ACC(table(p_ols68, test6$volact))),\n",
    "#ACC(table(p_ols69, test6$volact))),\n",
    "\n",
    "c(F1(table(p_ols61, test6$volact)),\n",
    "F1(table(p_ols62, test6$volact)),\n",
    "F1(table(p_ols63, test6$volact)),\n",
    "F1(table(p_ols64, test6$volact)),\n",
    "F1(table(p_ols65, test6$volact)),\n",
    "F1(table(p_ols66, test6$volact)),\n",
    "F1(table(p_ols67, test6$volact)),\n",
    "F1(table(p_ols68, test6$volact))))\n",
    "#F1(table(p_ols69, test6$volact))))\n",
    "\n",
    "max(threshold[,2])\n",
    "max(threshold[,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>5468</li>\n",
       "\t<li>51</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 5468\n",
       "\\item 51\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 5468\n",
       "2. 51\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 5468   51"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(test6c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1c <- train1c$volact\n",
    "X1c <- as.matrix(train1c[,-c(1, 44)])\n",
    "ols_model1c <- glm(y1c ~ X1c, family=binomial(link='logit'))\n",
    "X1c <- as.matrix(test1c[,-c(1, 44)])\n",
    "p_ols1c <- predict(ols_model1c, data.frame(X1c), type = \"response\")\n",
    "\n",
    "#table(p_ols1, test1$volact)\n",
    "#ACC(table(p_ols1, test1$volact))\n",
    "#F1(table(p_ols1, test1$volact))\n",
    "\n",
    "y6c <- train6c$volact\n",
    "X6c <- as.matrix(train6c[,-c(1, 51)])\n",
    "ols_model6c <- glm(y6c ~ X6c, family=binomial(link='logit'))\n",
    "X6c <- as.matrix(test6c[,-c(1, 51)])\n",
    "p_ols6c <- predict(ols_model6c, data.frame(X6c), type = \"response\")\n",
    "\n",
    "#table(y_hat6, test6$volact)\n",
    "#ACC(table(y_hat6, test6$volact))\n",
    "#F1(table(y_hat6, test6$volact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.716241737488196"
      ],
      "text/latex": [
       "0.716241737488196"
      ],
      "text/markdown": [
       "0.716241737488196"
      ],
      "text/plain": [
       "[1] 0.7162417375"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.600053433075073"
      ],
      "text/latex": [
       "0.600053433075073"
      ],
      "text/markdown": [
       "0.600053433075073"
      ],
      "text/plain": [
       "[1] 0.6000534331"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_ols11c <- ifelse(p_ols1c > 0.1, 1, 0)\n",
    "p_ols12c <- ifelse(p_ols1c > 0.2, 1, 0)\n",
    "p_ols13c <- ifelse(p_ols1c > 0.3, 1, 0)\n",
    "p_ols14c <- ifelse(p_ols1c > 0.4, 1, 0)\n",
    "p_ols15c <- ifelse(p_ols1c > 0.5, 1, 0)\n",
    "p_ols16c <- ifelse(p_ols1c > 0.6, 1, 0)\n",
    "p_ols17c <- ifelse(p_ols1c > 0.7, 1, 0)\n",
    "p_ols18c <- ifelse(p_ols1c > 0.8, 1, 0)\n",
    "#p_ols19c <- ifelse(p_ols1c > 0.9, 1, 0)\n",
    "\n",
    "threshold <- cbind(seq(0.1, 0.8, length.out = 8),\n",
    "    \n",
    "c(ACC(table(p_ols11c, test1c$volact)),\n",
    "ACC(table(p_ols12c, test1c$volact)),\n",
    "ACC(table(p_ols13c, test1c$volact)),\n",
    "ACC(table(p_ols14c, test1c$volact)),\n",
    "ACC(table(p_ols15c, test1c$volact)),\n",
    "ACC(table(p_ols16c, test1c$volact)),\n",
    "ACC(table(p_ols17c, test1c$volact)),\n",
    "ACC(table(p_ols18c, test1c$volact))),\n",
    "#ACC(table(p_ols19c, test1c$volact))),\n",
    "\n",
    "c(F1(table(p_ols11c, test1c$volact)),\n",
    "F1(table(p_ols12c, test1c$volact)),\n",
    "F1(table(p_ols13c, test1c$volact)),\n",
    "F1(table(p_ols14c, test1c$volact)),\n",
    "F1(table(p_ols15c, test1c$volact)),\n",
    "F1(table(p_ols16c, test1c$volact)),\n",
    "F1(table(p_ols17c, test1c$volact)),\n",
    "F1(table(p_ols18c, test1c$volact))))\n",
    "#F1(table(p_ols19c, test1c$volact))))\n",
    "\n",
    "max(threshold[,2])\n",
    "max(threshold[,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.698610095098756"
      ],
      "text/latex": [
       "0.698610095098756"
      ],
      "text/markdown": [
       "0.698610095098756"
      ],
      "text/plain": [
       "[1] 0.6986100951"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.602750647797489"
      ],
      "text/latex": [
       "0.602750647797489"
      ],
      "text/markdown": [
       "0.602750647797489"
      ],
      "text/plain": [
       "[1] 0.6027506478"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_ols61c <- ifelse(p_ols6c > 0.1, 1, 0)\n",
    "p_ols62c <- ifelse(p_ols6c > 0.2, 1, 0)\n",
    "p_ols63c <- ifelse(p_ols6c > 0.3, 1, 0)\n",
    "p_ols64c <- ifelse(p_ols6c > 0.4, 1, 0)\n",
    "p_ols65c <- ifelse(p_ols6c > 0.5, 1, 0)\n",
    "p_ols66c <- ifelse(p_ols6c > 0.6, 1, 0)\n",
    "p_ols67c <- ifelse(p_ols6c > 0.7, 1, 0)\n",
    "p_ols68c <- ifelse(p_ols6c > 0.8, 1, 0)\n",
    "#p_ols69c <- ifelse(p_ols6c > 0.9, 1, 0)\n",
    "\n",
    "threshold <- cbind(seq(0.1, 0.8, length.out = 8),\n",
    "    \n",
    "c(ACC(table(p_ols61c, test6c$volact)),\n",
    "ACC(table(p_ols62c, test6c$volact)),\n",
    "ACC(table(p_ols63c, test6c$volact)),\n",
    "ACC(table(p_ols64c, test6c$volact)),\n",
    "ACC(table(p_ols65c, test6c$volact)),\n",
    "ACC(table(p_ols66c, test6c$volact)),\n",
    "ACC(table(p_ols67c, test6c$volact)),\n",
    "ACC(table(p_ols68c, test6c$volact))),\n",
    "#ACC(table(p_ols69c, test6c$volact))),\n",
    "\n",
    "c(F1(table(p_ols61c, test6c$volact)),\n",
    "F1(table(p_ols62c, test6c$volact)),\n",
    "F1(table(p_ols63c, test6c$volact)),\n",
    "F1(table(p_ols64c, test6c$volact)),\n",
    "F1(table(p_ols65c, test6c$volact)),\n",
    "F1(table(p_ols66c, test6c$volact)),\n",
    "F1(table(p_ols67c, test6c$volact)),\n",
    "F1(table(p_ols68c, test6c$volact))))\n",
    "#F1(table(p_ols69c, test6c$volact))))\n",
    "\n",
    "max(threshold[,2])\n",
    "max(threshold[,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Connection successful!\n",
      "\n",
      "R is connected to the H2O cluster: \n",
      "    H2O cluster uptime:         1 hours 34 minutes \n",
      "    H2O cluster timezone:       Europe/Berlin \n",
      "    H2O data parsing timezone:  UTC \n",
      "    H2O cluster version:        3.18.0.8 \n",
      "    H2O cluster version age:    1 month and 17 days  \n",
      "    H2O cluster name:           H2O_started_from_R_laris_dzq933 \n",
      "    H2O cluster total nodes:    1 \n",
      "    H2O cluster total memory:   0.18 GB \n",
      "    H2O cluster total cores:    4 \n",
      "    H2O cluster allowed cores:  4 \n",
      "    H2O cluster healthy:        TRUE \n",
      "    H2O Connection ip:          localhost \n",
      "    H2O Connection port:        54321 \n",
      "    H2O Connection proxy:       NA \n",
      "    H2O Internal Security:      FALSE \n",
      "    H2O API Extensions:         Algos, AutoML, Core V3, Core V4 \n",
      "    R Version:                  R version 3.4.1 (2017-06-30) \n",
      "\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "Model Details:\n",
      "==============\n",
      "\n",
      "H2OBinomialModel: deeplearning\n",
      "Model Key:  DeepLearning_model_R_1528213959851_116 \n",
      "Status of Neuron Layers: predicting volact, 2-class classification, bernoulli distribution, CrossEntropy loss, 64.902 weights/biases, 773,7 KB, 381.230 training samples, mini-batch size 1\n",
      "  layer units      type dropout       l1       l2 mean_rate rate_rms momentum\n",
      "1     1    21     Input  0.00 %                                              \n",
      "2     2   200 Rectifier  0.00 % 0.000000 0.000000  0.020840 0.006848 0.000000\n",
      "3     3   200 Rectifier  0.00 % 0.000000 0.000000  0.009789 0.003917 0.000000\n",
      "4     4   100 Rectifier  0.00 % 0.000000 0.000000  0.025363 0.054854 0.000000\n",
      "5     5     2   Softmax         0.000000 0.000000  0.008679 0.004668 0.000000\n",
      "  mean_weight weight_rms mean_bias bias_rms\n",
      "1                                          \n",
      "2    0.011558   0.153449  0.342326 0.093147\n",
      "3   -0.025657   0.071072  0.925323 0.047194\n",
      "4   -0.004891   0.083306  0.010447 0.353857\n",
      "5   -0.014274   0.386104  0.008425 0.334626\n",
      "\n",
      "H2OBinomialMetrics: deeplearning\n",
      "** Reported on training data. **\n",
      "** Metrics reported on temporary training frame with 9874 samples **\n",
      "\n",
      "MSE:  0.2018255526\n",
      "RMSE:  0.449249989\n",
      "LogLoss:  0.5876310695\n",
      "Mean Per-Class Error:  0.369041894\n",
      "AUC:  0.6943079643\n",
      "Gini:  0.3886159285\n",
      "\n",
      "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
      "          0    1    Error        Rate\n",
      "0      2967 3569 0.546053  =3569/6536\n",
      "1       641 2697 0.192031   =641/3338\n",
      "Totals 3608 6266 0.426372  =4210/9874\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "                        metric threshold    value idx\n",
      "1                       max f1  0.316917 0.561641 249\n",
      "2                       max f2  0.181300 0.734545 336\n",
      "3                 max f0point5  0.457428 0.509554 150\n",
      "4                 max accuracy  0.528586 0.686956  98\n",
      "5                max precision  0.791924 0.750000   2\n",
      "6                   max recall  0.081673 1.000000 391\n",
      "7              max specificity  0.818107 0.999847   0\n",
      "8             max absolute_mcc  0.389849 0.270816 200\n",
      "9   max min_per_class_accuracy  0.401801 0.638107 192\n",
      "10 max mean_per_class_accuracy  0.389849 0.643004 200\n",
      "\n",
      "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
      "H2OBinomialMetrics: deeplearning\n",
      "** Reported on validation data. **\n",
      "** Metrics reported on full validation frame **\n",
      "\n",
      "MSE:  0.2056796463\n",
      "RMSE:  0.4535191797\n",
      "LogLoss:  0.5978529424\n",
      "Mean Per-Class Error:  0.378313709\n",
      "AUC:  0.6728345438\n",
      "Gini:  0.3456690877\n",
      "\n",
      "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
      "          0    1    Error        Rate\n",
      "0      1225 1616 0.568814  =1616/2841\n",
      "1       262 1133 0.187814   =262/1395\n",
      "Totals 1487 2749 0.443343  =1878/4236\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "                        metric threshold    value idx\n",
      "1                       max f1  0.313980 0.546815 249\n",
      "2                       max f2  0.138969 0.716527 361\n",
      "3                 max f0point5  0.399788 0.484238 188\n",
      "4                 max accuracy  0.519957 0.683192  99\n",
      "5                max precision  0.872911 1.000000   0\n",
      "6                   max recall  0.051913 1.000000 399\n",
      "7              max specificity  0.872911 1.000000   0\n",
      "8             max absolute_mcc  0.367871 0.254644 209\n",
      "9   max min_per_class_accuracy  0.402864 0.630412 186\n",
      "10 max mean_per_class_accuracy  0.382135 0.635423 200\n",
      "\n",
      "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
      "\n",
      "\n",
      "Scoring History: \n",
      "            timestamp   duration training_speed   epochs iterations\n",
      "1 2018-06-05 19:28:11  0.000 sec                 0.00000          0\n",
      "2 2018-06-05 19:28:17  8.535 sec   5569 obs/sec  1.00000          1\n",
      "3 2018-06-05 19:28:35 26.221 sec   8210 obs/sec  5.00000          5\n",
      "4 2018-06-05 19:28:51 42.167 sec   9073 obs/sec  9.00000          9\n",
      "5 2018-06-05 19:28:56 46.680 sec   9192 obs/sec 10.00000         10\n",
      "6 2018-06-05 19:28:57 47.985 sec   9186 obs/sec 10.00000         10\n",
      "        samples training_rmse training_logloss training_auc training_lift\n",
      "1      0.000000                                                          \n",
      "2  38123.000000       0.48025          0.66490      0.68257       1.91228\n",
      "3 190615.000000       0.44925          0.58763      0.69431       1.97204\n",
      "4 343107.000000       0.44104          0.57030      0.72078       2.33059\n",
      "5 381230.000000       0.43942          0.56565      0.72608       2.30071\n",
      "6 381230.000000       0.44925          0.58763      0.69431       1.97204\n",
      "  training_classification_error validation_rmse validation_logloss\n",
      "1                                                                 \n",
      "2                       0.43366         0.47638            0.65747\n",
      "3                       0.42637         0.45352            0.59785\n",
      "4                       0.38576         0.45411            0.59915\n",
      "5                       0.38556         0.45263            0.59960\n",
      "6                       0.42637         0.45352            0.59785\n",
      "  validation_auc validation_lift validation_classification_error\n",
      "1                                                               \n",
      "2        0.67254         1.83606                         0.40840\n",
      "3        0.67283         1.90668                         0.44334\n",
      "4        0.66903         1.55359                         0.48064\n",
      "5        0.67437         1.83606                         0.40250\n",
      "6        0.67283         1.90668                         0.44334\n",
      "\n",
      "Variable Importances: (Extract with `h2o.varimp`) \n",
      "=================================================\n",
      "\n",
      "Variable Importances: \n",
      "            variable relative_importance scaled_importance percentage\n",
      "1             eduyrs            1.000000          1.000000   0.070609\n",
      "2            sclmeet            0.983027          0.983027   0.069411\n",
      "3       social_trust            0.914753          0.914753   0.064590\n",
      "4 political_interest            0.804500          0.804500   0.056805\n",
      "5             wkhtot            0.794749          0.794749   0.056117\n",
      "\n",
      "---\n",
      "           variable relative_importance scaled_importance percentage\n",
      "16           church            0.582335          0.582335   0.041118\n",
      "17       solidarity            0.574953          0.574953   0.040597\n",
      "18        tolerance            0.543504          0.543504   0.038376\n",
      "19           trstep            0.524799          0.524799   0.037056\n",
      "20 self_realisation            0.500793          0.500793   0.035361\n",
      "21      houseperson            0.416933          0.416933   0.029439\n",
      "Model Details:\n",
      "==============\n",
      "\n",
      "H2OBinomialModel: deeplearning\n",
      "Model Key:  DeepLearning_model_R_1528213959851_127 \n",
      "Status of Neuron Layers: predicting volact, 2-class classification, bernoulli distribution, CrossEntropy loss, 64.902 weights/biases, 773,7 KB, 492.050 training samples, mini-batch size 1\n",
      "  layer units      type dropout       l1       l2 mean_rate rate_rms momentum\n",
      "1     1    21     Input  0.00 %                                              \n",
      "2     2   200 Rectifier  0.00 % 0.000000 0.000000  0.023454 0.009901 0.000000\n",
      "3     3   200 Rectifier  0.00 % 0.000000 0.000000  0.010612 0.004286 0.000000\n",
      "4     4   100 Rectifier  0.00 % 0.000000 0.000000  0.026269 0.049032 0.000000\n",
      "5     5     2   Softmax         0.000000 0.000000  0.010034 0.005787 0.000000\n",
      "  mean_weight weight_rms mean_bias bias_rms\n",
      "1                                          \n",
      "2    0.009012   0.145022  0.343891 0.102387\n",
      "3   -0.025885   0.070494  0.928753 0.048589\n",
      "4   -0.004786   0.083246  0.008091 0.323725\n",
      "5   -0.014331   0.385441  0.000699 0.247804\n",
      "\n",
      "H2OBinomialMetrics: deeplearning\n",
      "** Reported on training data. **\n",
      "** Metrics reported on temporary training frame with 9885 samples **\n",
      "\n",
      "MSE:  0.2008871408\n",
      "RMSE:  0.4482043516\n",
      "LogLoss:  0.5865053873\n",
      "Mean Per-Class Error:  0.3457616476\n",
      "AUC:  0.716741034\n",
      "Gini:  0.433482068\n",
      "\n",
      "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
      "          0    1    Error        Rate\n",
      "0      3594 2715 0.430338  =2715/6309\n",
      "1       934 2642 0.261186   =934/3576\n",
      "Totals 4528 5357 0.369145  =3649/9885\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "                        metric threshold    value idx\n",
      "1                       max f1  0.326696 0.591515 222\n",
      "2                       max f2  0.133108 0.749285 345\n",
      "3                 max f0point5  0.412041 0.564096 159\n",
      "4                 max accuracy  0.485732 0.694183 114\n",
      "5                max precision  0.663874 0.787356  25\n",
      "6                   max recall  0.047230 1.000000 392\n",
      "7              max specificity  0.775699 0.999841   0\n",
      "8             max absolute_mcc  0.400836 0.309800 167\n",
      "9   max min_per_class_accuracy  0.357952 0.657632 199\n",
      "10 max mean_per_class_accuracy  0.357952 0.659073 199\n",
      "\n",
      "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
      "H2OBinomialMetrics: deeplearning\n",
      "** Reported on validation data. **\n",
      "** Metrics reported on full validation frame **\n",
      "\n",
      "MSE:  0.206111642\n",
      "RMSE:  0.4539952005\n",
      "LogLoss:  0.6012335449\n",
      "Mean Per-Class Error:  0.3517130964\n",
      "AUC:  0.7013677943\n",
      "Gini:  0.4027355886\n",
      "\n",
      "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
      "          0    1    Error        Rate\n",
      "0      1993 1477 0.425648  =1477/3470\n",
      "1       555 1443 0.277778   =555/1998\n",
      "Totals 2548 2920 0.371617  =2032/5468\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "                        metric threshold    value idx\n",
      "1                       max f1  0.328002 0.586824 224\n",
      "2                       max f2  0.110469 0.745261 359\n",
      "3                 max f0point5  0.391926 0.553589 176\n",
      "4                 max accuracy  0.488565 0.684711 113\n",
      "5                max precision  0.757063 1.000000   0\n",
      "6                   max recall  0.031850 1.000000 397\n",
      "7              max specificity  0.757063 1.000000   0\n",
      "8             max absolute_mcc  0.376074 0.303262 187\n",
      "9   max min_per_class_accuracy  0.357372 0.651151 202\n",
      "10 max mean_per_class_accuracy  0.364332 0.656131 197\n",
      "\n",
      "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
      "\n",
      "\n",
      "Scoring History: \n",
      "            timestamp   duration training_speed   epochs iterations\n",
      "1 2018-06-05 19:29:00  0.000 sec                 0.00000          0\n",
      "2 2018-06-05 19:29:08  9.969 sec   6041 obs/sec  1.00000          1\n",
      "3 2018-06-05 19:29:26 27.131 sec   8159 obs/sec  4.00000          4\n",
      "4 2018-06-05 19:29:41 42.002 sec   9091 obs/sec  7.00000          7\n",
      "5 2018-06-05 19:29:57 58.784 sec   9199 obs/sec 10.00000         10\n",
      "6 2018-06-05 19:29:59 59.965 sec   9195 obs/sec 10.00000         10\n",
      "        samples training_rmse training_logloss training_auc training_lift\n",
      "1      0.000000                                                          \n",
      "2  49205.000000       0.45478          0.60043      0.70498       2.06622\n",
      "3 196820.000000       0.44820          0.58651      0.71674       2.17790\n",
      "4 344435.000000       0.44590          0.58201      0.72176       2.17790\n",
      "5 492050.000000       0.44258          0.57338      0.73262       2.28959\n",
      "6 492050.000000       0.44820          0.58651      0.71674       2.17790\n",
      "  training_classification_error validation_rmse validation_logloss\n",
      "1                                                                 \n",
      "2                       0.36449         0.45798            0.60762\n",
      "3                       0.36915         0.45400            0.60123\n",
      "4                       0.41012         0.45410            0.60316\n",
      "5                       0.37491         0.45533            0.60464\n",
      "6                       0.36915         0.45400            0.60123\n",
      "  validation_auc validation_lift validation_classification_error\n",
      "1                                                               \n",
      "2        0.69479         2.18939                         0.39704\n",
      "3        0.70137         2.04011                         0.37162\n",
      "4        0.70142         2.08987                         0.41789\n",
      "5        0.69633         2.08987                         0.41405\n",
      "6        0.70137         2.04011                         0.37162\n",
      "\n",
      "Variable Importances: (Extract with `h2o.varimp`) \n",
      "=================================================\n",
      "\n",
      "Variable Importances: \n",
      "    variable relative_importance scaled_importance percentage\n",
      "1      yrbrn            1.000000          1.000000   0.064118\n",
      "2     eduyrs            0.996734          0.996734   0.063909\n",
      "3    sclmeet            0.942215          0.942215   0.060413\n",
      "4     church            0.910162          0.910162   0.058358\n",
      "5 solidarity            0.871860          0.871860   0.055902\n",
      "\n",
      "---\n",
      "           variable relative_importance scaled_importance percentage\n",
      "16           stfdem            0.635124          0.635124   0.040723\n",
      "17           trstep            0.630635          0.630635   0.040435\n",
      "18      houseperson            0.612557          0.612557   0.039276\n",
      "19 self_realisation            0.570990          0.570990   0.036611\n",
      "20           wkhtot            0.566339          0.566339   0.036313\n",
      "21        tolerance            0.512236          0.512236   0.032844\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.557365439093484"
      ],
      "text/latex": [
       "0.557365439093484"
      ],
      "text/markdown": [
       "0.557365439093484"
      ],
      "text/plain": [
       "[1] 0.5573654391"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.546553808948005"
      ],
      "text/latex": [
       "0.546553808948005"
      ],
      "text/markdown": [
       "0.546553808948005"
      ],
      "text/plain": [
       "[1] 0.5465538089"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.628200438917337"
      ],
      "text/latex": [
       "0.628200438917337"
      ],
      "text/markdown": [
       "0.628200438917337"
      ],
      "text/plain": [
       "[1] 0.6282004389"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.585862701161133"
      ],
      "text/latex": [
       "0.585862701161133"
      ],
      "text/markdown": [
       "0.585862701161133"
      ],
      "text/plain": [
       "[1] 0.5858627012"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "Model Details:\n",
      "==============\n",
      "\n",
      "H2OBinomialModel: deeplearning\n",
      "Model Key:  DeepLearning_model_R_1528213959851_140 \n",
      "Status of Neuron Layers: predicting volact, 2-class classification, bernoulli distribution, CrossEntropy loss, 69.302 weights/biases, 828,8 KB, 381.230 training samples, mini-batch size 1\n",
      "  layer units      type dropout       l1       l2 mean_rate rate_rms momentum\n",
      "1     1    43     Input  0.00 %                                              \n",
      "2     2   200 Rectifier  0.00 % 0.000000 0.000000  0.019676 0.021446 0.000000\n",
      "3     3   200 Rectifier  0.00 % 0.000000 0.000000  0.008029 0.006521 0.000000\n",
      "4     4   100 Rectifier  0.00 % 0.000000 0.000000  0.032347 0.056819 0.000000\n",
      "5     5     2   Softmax         0.000000 0.000000  0.006148 0.002844 0.000000\n",
      "  mean_weight weight_rms mean_bias bias_rms\n",
      "1                                          \n",
      "2   -0.063611   0.233349  0.378600 0.121440\n",
      "3   -0.026028   0.074160  0.911883 0.081197\n",
      "4   -0.005623   0.085527 -0.149180 0.315225\n",
      "5   -0.014570   0.374575 -0.013481 0.294774\n",
      "\n",
      "H2OBinomialMetrics: deeplearning\n",
      "** Reported on training data. **\n",
      "** Metrics reported on temporary training frame with 9874 samples **\n",
      "\n",
      "MSE:  0.1746607752\n",
      "RMSE:  0.4179243655\n",
      "LogLoss:  0.5123485614\n",
      "Mean Per-Class Error:  0.2915051119\n",
      "AUC:  0.7843017022\n",
      "Gini:  0.5686034044\n",
      "\n",
      "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
      "          0    1    Error        Rate\n",
      "0      3820 2716 0.415545  =2716/6536\n",
      "1       559 2779 0.167466   =559/3338\n",
      "Totals 4379 5495 0.331679  =3275/9874\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "                        metric threshold    value idx\n",
      "1                       max f1  0.292230 0.629231 252\n",
      "2                       max f2  0.136803 0.772858 332\n",
      "3                 max f0point5  0.436938 0.597803 169\n",
      "4                 max accuracy  0.499335 0.732226 139\n",
      "5                max precision  0.963758 1.000000   0\n",
      "6                   max recall  0.021455 1.000000 394\n",
      "7              max specificity  0.963758 1.000000   0\n",
      "8             max absolute_mcc  0.325814 0.397705 232\n",
      "9   max min_per_class_accuracy  0.367184 0.698922 208\n",
      "10 max mean_per_class_accuracy  0.325814 0.710172 232\n",
      "\n",
      "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
      "H2OBinomialMetrics: deeplearning\n",
      "** Reported on validation data. **\n",
      "** Metrics reported on full validation frame **\n",
      "\n",
      "MSE:  0.1840044468\n",
      "RMSE:  0.4289573951\n",
      "LogLoss:  0.5372203685\n",
      "Mean Per-Class Error:  0.3162925367\n",
      "AUC:  0.7501133555\n",
      "Gini:  0.500226711\n",
      "\n",
      "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
      "          0    1    Error        Rate\n",
      "0      1722 1119 0.393875  =1119/2841\n",
      "1       333 1062 0.238710   =333/1395\n",
      "Totals 2055 2181 0.342776  =1452/4236\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "                        metric threshold    value idx\n",
      "1                       max f1  0.311363 0.593960 236\n",
      "2                       max f2  0.081694 0.751071 358\n",
      "3                 max f0point5  0.438117 0.551796 162\n",
      "4                 max accuracy  0.497915 0.717422 134\n",
      "5                max precision  0.964500 1.000000   0\n",
      "6                   max recall  0.024797 1.000000 392\n",
      "7              max specificity  0.964500 1.000000   0\n",
      "8             max absolute_mcc  0.311363 0.345498 236\n",
      "9   max min_per_class_accuracy  0.358917 0.675269 210\n",
      "10 max mean_per_class_accuracy  0.311363 0.683707 236\n",
      "\n",
      "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
      "\n",
      "\n",
      "Scoring History: \n",
      "            timestamp          duration training_speed   epochs iterations\n",
      "1 2018-06-05 19:30:06         0.000 sec                 0.00000          0\n",
      "2 2018-06-05 19:30:14        10.395 sec   4488 obs/sec  1.00000          1\n",
      "3 2018-06-05 19:30:28        24.187 sec   5465 obs/sec  3.00000          3\n",
      "4 2018-06-05 19:30:45        41.402 sec   6168 obs/sec  6.00000          6\n",
      "5 2018-06-05 19:30:56        52.532 sec   6479 obs/sec  8.00000          8\n",
      "6 2018-06-05 19:31:07  1 min  3.517 sec   6685 obs/sec 10.00000         10\n",
      "7 2018-06-05 19:31:09  1 min  4.992 sec   6676 obs/sec 10.00000         10\n",
      "        samples training_rmse training_logloss training_auc training_lift\n",
      "1      0.000000                                                          \n",
      "2  38123.000000       0.48144          0.66089      0.74984       2.30071\n",
      "3 114369.000000       0.43318          0.54513      0.75763       2.33059\n",
      "4 228738.000000       0.42779          0.53318      0.77458       2.53975\n",
      "5 304984.000000       0.41792          0.51235      0.78430       2.59951\n",
      "6 381230.000000       0.41641          0.50917      0.79283       2.59951\n",
      "7 381230.000000       0.41792          0.51235      0.78430       2.59951\n",
      "  training_classification_error validation_rmse validation_logloss\n",
      "1                                                                 \n",
      "2                       0.36540         0.47679            0.65481\n",
      "3                       0.34738         0.43763            0.55425\n",
      "4                       0.33968         0.43311            0.54784\n",
      "5                       0.33168         0.42896            0.53722\n",
      "6                       0.33300         0.43358            0.54960\n",
      "7                       0.33168         0.42896            0.53722\n",
      "  validation_auc validation_lift validation_classification_error\n",
      "1                                                               \n",
      "2        0.74137         2.68347                         0.35505\n",
      "3        0.74083         2.40100                         0.35458\n",
      "4        0.74927         2.54224                         0.36615\n",
      "5        0.75011         2.68347                         0.34278\n",
      "6        0.74192         2.68347                         0.36992\n",
      "7        0.75011         2.68347                         0.34278\n",
      "\n",
      "Variable Importances: (Extract with `h2o.varimp`) \n",
      "=================================================\n",
      "\n",
      "Variable Importances: \n",
      "  variable relative_importance scaled_importance percentage\n",
      "1  cntryPL            1.000000          1.000000   0.047244\n",
      "2  cntryGR            0.957528          0.957528   0.045237\n",
      "3  cntryPT            0.731378          0.731378   0.034553\n",
      "4  cntryCH            0.719629          0.719629   0.033998\n",
      "5  cntryHU            0.715682          0.715682   0.033812\n",
      "\n",
      "---\n",
      "           variable relative_importance scaled_importance percentage\n",
      "38          cntryDK            0.333342          0.333342   0.015748\n",
      "39        tolerance            0.331614          0.331614   0.015667\n",
      "40           trstep            0.324998          0.324998   0.015354\n",
      "41        trust_leg            0.322920          0.322920   0.015256\n",
      "42      houseperson            0.299717          0.299717   0.014160\n",
      "43 self_realisation            0.285885          0.285885   0.013506\n",
      "Model Details:\n",
      "==============\n",
      "\n",
      "H2OBinomialModel: deeplearning\n",
      "Model Key:  DeepLearning_model_R_1528213959851_153 \n",
      "Status of Neuron Layers: predicting volact, 2-class classification, bernoulli distribution, CrossEntropy loss, 70.702 weights/biases, 846,3 KB, 492.050 training samples, mini-batch size 1\n",
      "  layer units      type dropout       l1       l2 mean_rate rate_rms momentum\n",
      "1     1    50     Input  0.00 %                                              \n",
      "2     2   200 Rectifier  0.00 % 0.000000 0.000000  0.022935 0.018378 0.000000\n",
      "3     3   200 Rectifier  0.00 % 0.000000 0.000000  0.008409 0.004256 0.000000\n",
      "4     4   100 Rectifier  0.00 % 0.000000 0.000000  0.028916 0.042497 0.000000\n",
      "5     5     2   Softmax         0.000000 0.000000  0.005161 0.002929 0.000000\n",
      "  mean_weight weight_rms mean_bias bias_rms\n",
      "1                                          \n",
      "2   -0.091516   0.307693  0.280658 0.141102\n",
      "3   -0.027867   0.077889  0.898228 0.121698\n",
      "4   -0.006326   0.085647 -0.272993 0.330904\n",
      "5   -0.014324   0.368588 -0.005547 0.478716\n",
      "\n",
      "H2OBinomialMetrics: deeplearning\n",
      "** Reported on training data. **\n",
      "** Metrics reported on temporary training frame with 9885 samples **\n",
      "\n",
      "MSE:  0.1825790786\n",
      "RMSE:  0.4272927318\n",
      "LogLoss:  0.5402010397\n",
      "Mean Per-Class Error:  0.2975327672\n",
      "AUC:  0.7730315087\n",
      "Gini:  0.5460630175\n",
      "\n",
      "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
      "          0    1    Error        Rate\n",
      "0      3825 2484 0.393723  =2484/6309\n",
      "1       720 2856 0.201342   =720/3576\n",
      "Totals 4545 5340 0.324127  =3204/9885\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "                        metric threshold    value idx\n",
      "1                       max f1  0.304822 0.640646 244\n",
      "2                       max f2  0.160127 0.769300 322\n",
      "3                 max f0point5  0.460119 0.613467 163\n",
      "4                 max accuracy  0.460119 0.721396 163\n",
      "5                max precision  0.940235 1.000000   0\n",
      "6                   max recall  0.023225 1.000000 392\n",
      "7              max specificity  0.940235 1.000000   0\n",
      "8             max absolute_mcc  0.308486 0.390840 242\n",
      "9   max min_per_class_accuracy  0.359326 0.698826 216\n",
      "10 max mean_per_class_accuracy  0.308486 0.702897 242\n",
      "\n",
      "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
      "H2OBinomialMetrics: deeplearning\n",
      "** Reported on validation data. **\n",
      "** Metrics reported on full validation frame **\n",
      "\n",
      "MSE:  0.1942840633\n",
      "RMSE:  0.4407766592\n",
      "LogLoss:  0.5728468616\n",
      "Mean Per-Class Error:  0.325405665\n",
      "AUC:  0.738949324\n",
      "Gini:  0.4778986479\n",
      "\n",
      "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
      "          0    1    Error        Rate\n",
      "0      2087 1383 0.398559  =1383/3470\n",
      "1       504 1494 0.252252   =504/1998\n",
      "Totals 2591 2877 0.345099  =1887/5468\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "                        metric threshold    value idx\n",
      "1                       max f1  0.314348 0.612923 242\n",
      "2                       max f2  0.138501 0.757564 332\n",
      "3                 max f0point5  0.462926 0.587029 165\n",
      "4                 max accuracy  0.483254 0.705194 154\n",
      "5                max precision  0.927485 1.000000   0\n",
      "6                   max recall  0.011656 1.000000 395\n",
      "7              max specificity  0.927485 1.000000   0\n",
      "8             max absolute_mcc  0.436353 0.340581 179\n",
      "9   max min_per_class_accuracy  0.353703 0.672673 219\n",
      "10 max mean_per_class_accuracy  0.325785 0.676127 234\n",
      "\n",
      "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
      "\n",
      "\n",
      "Scoring History: \n",
      "            timestamp          duration training_speed   epochs iterations\n",
      "1 2018-06-05 19:31:12         0.000 sec                 0.00000          0\n",
      "2 2018-06-05 19:31:22        12.788 sec   4654 obs/sec  1.00000          1\n",
      "3 2018-06-05 19:31:40        30.013 sec   5523 obs/sec  3.00000          3\n",
      "4 2018-06-05 19:31:55        44.854 sec   6100 obs/sec  5.00000          5\n",
      "5 2018-06-05 19:32:09        59.783 sec   6388 obs/sec  7.00000          7\n",
      "6 2018-06-05 19:32:23  1 min 13.762 sec   6650 obs/sec  9.00000          9\n",
      "7 2018-06-05 19:32:31  1 min 21.182 sec   6762 obs/sec 10.00000         10\n",
      "8 2018-06-05 19:32:32  1 min 22.528 sec   6736 obs/sec 10.00000         10\n",
      "        samples training_rmse training_logloss training_auc training_lift\n",
      "1      0.000000                                                          \n",
      "2  49205.000000       0.44984          0.58653      0.73773       2.34543\n",
      "3 147615.000000       0.43863          0.56522      0.74562       2.34543\n",
      "4 246025.000000       0.43918          0.56603      0.75525       2.34543\n",
      "5 344435.000000       0.43198          0.55080      0.76159       2.34543\n",
      "6 442845.000000       0.42729          0.54020      0.77303       2.51297\n",
      "7 492050.000000       0.42573          0.53683      0.77677       2.48504\n",
      "8 492050.000000       0.42729          0.54020      0.77303       2.51297\n",
      "  training_classification_error validation_rmse validation_logloss\n",
      "1                                                                 \n",
      "2                       0.37633         0.45351            0.59631\n",
      "3                       0.36520         0.44214            0.57430\n",
      "4                       0.35336         0.44463            0.57899\n",
      "5                       0.33606         0.44167            0.57471\n",
      "6                       0.32413         0.44078            0.57285\n",
      "7                       0.32696         0.44153            0.57520\n",
      "8                       0.32413         0.44078            0.57285\n",
      "  validation_auc validation_lift validation_classification_error\n",
      "1                                                               \n",
      "2        0.72948         2.43818                         0.36887\n",
      "3        0.73532         2.48794                         0.35827\n",
      "4        0.73994         2.38842                         0.35077\n",
      "5        0.73836         2.43818                         0.34400\n",
      "6        0.73895         2.33867                         0.34510\n",
      "7        0.73786         2.38842                         0.33906\n",
      "8        0.73895         2.33867                         0.34510\n",
      "\n",
      "Variable Importances: (Extract with `h2o.varimp`) \n",
      "=================================================\n",
      "\n",
      "Variable Importances: \n",
      "  variable relative_importance scaled_importance percentage\n",
      "1  cntryBG            1.000000          1.000000   0.036425\n",
      "2  cntryUA            0.925617          0.925617   0.033716\n",
      "3  cntryEE            0.906240          0.906240   0.033010\n",
      "4  cntryPL            0.904871          0.904871   0.032960\n",
      "5  cntryLT            0.810020          0.810020   0.029505\n",
      "\n",
      "---\n",
      "      variable relative_importance scaled_importance percentage\n",
      "45   trust_exe            0.317881          0.317881   0.011579\n",
      "46   tolerance            0.317810          0.317810   0.011576\n",
      "47     cntryIS            0.302155          0.302155   0.011006\n",
      "48      wkhtot            0.295935          0.295935   0.010780\n",
      "49 houseperson            0.274325          0.274325   0.009992\n",
      "50     cntryCH            0.235089          0.235089   0.008563\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.657695939565628"
      ],
      "text/latex": [
       "0.657695939565628"
      ],
      "text/markdown": [
       "0.657695939565628"
      ],
      "text/plain": [
       "[1] 0.6576959396"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.593837535014006"
      ],
      "text/latex": [
       "0.593837535014006"
      ],
      "text/markdown": [
       "0.593837535014006"
      ],
      "text/plain": [
       "[1] 0.593837535"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.654901243599122"
      ],
      "text/latex": [
       "0.654901243599122"
      ],
      "text/markdown": [
       "0.654901243599122"
      ],
      "text/plain": [
       "[1] 0.6549012436"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.612446087492298"
      ],
      "text/latex": [
       "0.612446087492298"
      ],
      "text/markdown": [
       "0.612446087492298"
      ],
      "text/plain": [
       "[1] 0.6124460875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |===========================================                           |  61%\n",
      "\n",
      "water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for DRF model: forest1.  Details: ERRR on field: _ntrees: The tree model will not fit in the driver node's memory (93,2 KB per tree x 300 > 9,7 MB) - try decreasing ntrees and/or max_depth or increasing min_rows!\n",
      "\n",
      "\n",
      "water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for DRF model: forest1.  Details: ERRR on field: _ntrees: The tree model will not fit in the driver node's memory (93,2 KB per tree x 300 > 9,7 MB) - try decreasing ntrees and/or max_depth or increasing min_rows!\n",
      "\n",
      "\tat water.exceptions.H2OModelBuilderIllegalArgumentException.makeFromBuilder(H2OModelBuilderIllegalArgumentException.java:20)\n",
      "\tat hex.tree.SharedTree.doScoringAndSaveModel(SharedTree.java:674)\n",
      "\tat hex.tree.SharedTree$Driver.scoreAndBuildTrees(SharedTree.java:424)\n",
      "\tat hex.tree.SharedTree$Driver.computeImpl(SharedTree.java:356)\n",
      "\tat hex.ModelBuilder$Driver.compute2(ModelBuilder.java:206)\n",
      "\tat water.H2O$H2OCountedCompleter.compute(H2O.java:1263)\n",
      "\tat jsr166y.CountedCompleter.exec(CountedCompleter.java:468)\n",
      "\tat jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)\n",
      "\tat jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)\n",
      "\tat jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)\n",
      "\tat jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n",
      "\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for DRF model: forest1.  Details: ERRR on field: _ntrees: The tree model will not fit in the driver node's memory (93,2 KB per tree x 300 > 9,7 MB) - try decreasing ntrees and/or max_depth or increasing min_rows!\n",
     "execution_count": 125,
     "output_type": "error",
     "traceback": [
      "Error: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for DRF model: forest1.  Details: ERRR on field: _ntrees: The tree model will not fit in the driver node's memory (93,2 KB per tree x 300 > 9,7 MB) - try decreasing ntrees and/or max_depth or increasing min_rows!\nTraceback:\n",
      "1. h2o.randomForest(training_frame = h2o.train1, validation_frame = h2o.test1, \n .     x = xnames1, y = \"volact\", model_id = \"forest1\", ntrees = 300, \n .     max_depth = 50, stopping_rounds = 2, score_each_iteration = F, \n .     seed = 1000000)",
      "2. .h2o.modelJob(\"drf\", parms, h2oRestApiVersion = 3, verbose = verbose)",
      "3. h2o.getFutureModel(job, verbose = verbose)",
      "4. .h2o.__waitOnJob(object@job_key, verboseModelScoringHistory = verbose)",
      "5. tryCatch({\n .     while (keepRunning) {\n .         myJobUrlSuffix <- paste0(.h2o.__JOBS, \"/\", job_key)\n .         rawResponse <- .h2o.doSafeGET(urlSuffix = myJobUrlSuffix)\n .         jsonObject <- .h2o.fromJSON(jsonlite::fromJSON(rawResponse, \n .             simplifyDataFrame = FALSE))\n .         jobs <- jsonObject$jobs\n .         if (length(jobs) > 1) {\n .             stop(\"Job list has more than 1 entry\")\n .         }\n .         else if (length(jobs) == 0) {\n .             stop(\"Job list is empty\")\n .         }\n .         job = jobs[[1]]\n .         status = job$status\n .         stopifnot(is.character(status))\n .         if (status == \"FAILED\") {\n .             cat(\"\\n\\n\")\n .             cat(job$exception)\n .             cat(\"\\n\\n\")\n .             if (!is.null(job$stacktrace)) {\n .                 cat(job$stacktrace)\n .             }\n .             cat(\"\\n\")\n .             m <- strsplit(jobs[[1]]$exception, \"\\n\")[[1]][1]\n .             m <- gsub(\".*msg \", \"\", m)\n .             stop(m, call. = FALSE)\n .         }\n .         if (status == \"CANCELLED\") {\n .             stop(\"Job key \", job_key, \" cancelled by user\")\n .         }\n .         key = job$key\n .         name = key$name\n .         if (name != job_key) {\n .             message <- sprintf(\"Job %s not found in job list\", \n .                 job_key)\n .             stop(message)\n .         }\n .         if (progressBar) {\n .             progress = job$progress\n .             if (is.numeric(progress)) {\n .                 setTxtProgressBar(pb, progress)\n .             }\n .         }\n .         if ((status == \"CREATED\") || (status == \"RUNNING\")) {\n .         }\n .         else {\n .             stopifnot(status == \"DONE\")\n .             keepRunning <- FALSE\n .         }\n .         if (keepRunning) {\n .             Sys.sleep(pollInterval)\n .             if (verboseModelScoringHistory) {\n .                 cat(paste0(\"\\nScoring History for Model \", job$dest$name, \n .                   \" at \", Sys.time(), \"\\n\"))\n .                 print(paste0(\"Model Build is \", job$progress * \n .                   100, \"% done...\"))\n .                 if (!is.null(job$progress_msg)) {\n .                   print(tail(h2o.getModel(job$dest$name)@model$scoring_history))\n .                 }\n .                 else {\n .                   print(\"Scoring history is not available yet...\")\n .                 }\n .             }\n .         }\n .         else {\n .             if (progressBar) {\n .                 close(pb)\n .             }\n .             for (w in job$warnings) {\n .                 warning(w)\n .             }\n .         }\n .     }\n . }, interrupt = function(x) {\n .     url.suf <- paste0(.h2o.__JOBS, \"/\", job_key, \"/cancel\")\n .     .h2o.doSafePOST(urlSuffix = url.suf)\n .     message(paste0(\"\\nJob \", job_key, \" was cancelled.\\n\"))\n .     return()\n . })",
      "6. tryCatchList(expr, classes, parentenv, handlers)",
      "7. tryCatchOne(expr, names, parentenv, handlers[[1L]])",
      "8. doTryCatch(return(expr), name, parentenv, handler)",
      "9. stop(m, call. = FALSE)"
     ]
    }
   ],
   "source": [
    "train1$volact <- as.factor(train1$volact)\n",
    "test1$volact <- as.factor(test1$volact)\n",
    "train6$volact <- as.factor(train6$volact)\n",
    "test6$volact <- as.factor(test6$volact)\n",
    "\n",
    "h2o.train1 <- as.h2o(train1)\n",
    "h2o.test1 <- as.h2o(test1)\n",
    "h2o.train6 <- as.h2o(train6)\n",
    "h2o.test6 <- as.h2o(test6)\n",
    "\n",
    "train1c$volact <- as.factor(train1c$volact)\n",
    "test1c$volact <- as.factor(test1c$volact)\n",
    "train6c$volact <- as.factor(train6c$volact)\n",
    "test6c$volact <- as.factor(test6c$volact)\n",
    "\n",
    "h2o.train1c <- as.h2o(train1c)\n",
    "h2o.test1c <- as.h2o(test1c)\n",
    "h2o.train6c <- as.h2o(train6c)\n",
    "h2o.test6c <- as.h2o(test6c)\n",
    "\n",
    "xnames1 <- colnames(train1[,-1])\n",
    "xnames6 <- colnames(train6[,-1])\n",
    "\n",
    "xnames1c <- colnames(train1c[,-1])\n",
    "xnames6c <- colnames(train6c[,-1])\n",
    "\n",
    "nn1_1 <- h2o.deeplearning(\n",
    "  x = xnames1,\n",
    "  y = \"volact\",\n",
    "  training_frame = h2o.train1,\n",
    "  validation_frame = h2o.test1,\n",
    "  activation = \"Rectifier\",\n",
    "  hidden = c(200, 200, 100),\n",
    "  epochs = 10,\n",
    "  rate = .005,\n",
    "  loss = \"CrossEntropy\",\n",
    "  export_weights_and_biases = TRUE,\n",
    "  seed = 1000000\n",
    ")\n",
    "\n",
    "nn6_1 <- h2o.deeplearning(\n",
    "  x = xnames6,\n",
    "  y = \"volact\",\n",
    "  training_frame = h2o.train6,\n",
    "  validation_frame = h2o.test6,\n",
    "  activation = \"Rectifier\",\n",
    "  hidden = c(200, 200, 100),\n",
    "  epochs = 10,\n",
    "  rate = .005,\n",
    "  loss = \"CrossEntropy\",\n",
    "  export_weights_and_biases = TRUE, \n",
    "  seed = 1000000\n",
    ")\n",
    "\n",
    "summary(nn1_1)\n",
    "\n",
    "summary(nn6_1)\n",
    "\n",
    "nn1_1_pred <- h2o.predict(nn1_1, h2o.test1)\n",
    "nn6_1_pred <- h2o.predict(nn6_1, h2o.test6)\n",
    "ACC(table(as.vector(nn1_1_pred$predict), test1$volact))\n",
    "F1(table(as.vector(nn1_1_pred$predict), test1$volact))\n",
    "ACC(table(as.vector(nn6_1_pred$predict), test6$volact))\n",
    "F1(table(as.vector(nn6_1_pred$predict), test6$volact))\n",
    "\n",
    "nn1_1c <- h2o.deeplearning(\n",
    "  x = xnames1c,\n",
    "  y = \"volact\",\n",
    "  training_frame = h2o.train1c,\n",
    "  validation_frame = h2o.test1c,\n",
    "  activation = \"Rectifier\",\n",
    "  hidden = c(200, 200, 100),\n",
    "  epochs = 10,\n",
    "  rate = .005,\n",
    "  loss = \"CrossEntropy\",\n",
    "  export_weights_and_biases = TRUE,\n",
    "  seed = 1000000\n",
    ")\n",
    "\n",
    "nn6_1c <- h2o.deeplearning(\n",
    "  x = xnames6c,\n",
    "  y = \"volact\",\n",
    "  training_frame = h2o.train6c,\n",
    "  validation_frame = h2o.test6c,\n",
    "  activation = \"Rectifier\",\n",
    "  hidden = c(200, 200, 100),\n",
    "  epochs = 10,\n",
    "  rate = .005,\n",
    "  loss = \"CrossEntropy\",\n",
    "  export_weights_and_biases = TRUE, \n",
    "  seed = 1000000\n",
    ")\n",
    "\n",
    "summary(nn1_1c)\n",
    "\n",
    "summary(nn6_1c)\n",
    "\n",
    "nn1_1c_pred <- h2o.predict(nn1_1c, h2o.test1c)\n",
    "nn6_1c_pred <- h2o.predict(nn6_1c, h2o.test6c)\n",
    "ACC(table(as.vector(nn1_1c_pred$predict), test1c$volact))\n",
    "F1(table(as.vector(nn1_1c_pred$predict), test1c$volact))\n",
    "ACC(table(as.vector(nn6_1c_pred$predict), test6c$volact))\n",
    "F1(table(as.vector(nn6_1c_pred$predict), test6c$volact))\n",
    "\n",
    "rf1_1 <- h2o.randomForest(        \n",
    "  training_frame = h2o.train1,        \n",
    "  validation_frame = h2o.test1,      \n",
    "  x = xnames1,                        \n",
    "  y = \"volact\",                          \n",
    "  model_id = \"forest1\",    \n",
    "  ntrees = 300,            \n",
    "  max_depth = 50,\n",
    "  stopping_rounds = 2,           \n",
    "  score_each_iteration = F,     \n",
    "  seed = 1000000) \n",
    "\n",
    "rf6_1 <- h2o.randomForest(        \n",
    "  training_frame = h2o.train6,        \n",
    "  validation_frame = h2o.test6,      \n",
    "  x = xnames6,                        \n",
    "  y = \"volact\",                          \n",
    "  model_id = \"forest1\",    \n",
    "  ntrees = 300,            \n",
    "  max_depth = 50,\n",
    "  stopping_rounds = 2,           \n",
    "  score_each_iteration = F,     \n",
    "  seed = 1000000) \n",
    "\n",
    "summary(rf1_1)\n",
    "\n",
    "summary(rf6_1)\n",
    "\n",
    "rf1_1_pred <- h2o.predict(rf1_1, h2o.test1)\n",
    "rf6_1_pred <- h2o.predict(rf6_1, h2o.test6)\n",
    "ACC(table(as.vector(rf1_1_pred$predict), test1$volact))\n",
    "F1(table(as.vector(rf1_1_pred$predict), test1$volact))\n",
    "ACC(table(as.vector(rf6_1_pred$predict), test6$volact))\n",
    "F1(table(as.vector(rf6_1_pred$predict), test6$volact))\n",
    "\n",
    "rf1_1c <- h2o.randomForest(        \n",
    "  training_frame = h2o.train1c,        \n",
    "  validation_frame = h2o.test1c,      \n",
    "  x = xnames1c,                        \n",
    "  y = \"volact\",                          \n",
    "  model_id = \"forest1\",    \n",
    "  ntrees = 300,            \n",
    "  max_depth = 50,\n",
    "  stopping_rounds = 2,           \n",
    "  score_each_iteration = F,     \n",
    "  seed = 1000000) \n",
    "\n",
    "rf6_1c <- h2o.randomForest(        \n",
    "  training_frame = h2o.train6c,        \n",
    "  validation_frame = h2o.test6c,      \n",
    "  x = xnames6c,                        \n",
    "  y = \"volact\",                          \n",
    "  model_id = \"forest1\",    \n",
    "  ntrees = 300,            \n",
    "  max_depth = 50,\n",
    "  stopping_rounds = 2,           \n",
    "  score_each_iteration = F,     \n",
    "  seed = 1000000) \n",
    "\n",
    "summary(rf1_1c)\n",
    "\n",
    "summary(rf6_1c)\n",
    "\n",
    "rf1_1c_pred <- h2o.predict(rf1_1c, h2o.test1c)\n",
    "rf6_1c_pred <- h2o.predict(rf6_1c, h2o.test6c)\n",
    "ACC(table(as.vector(rf1_1c_pred$predict), test1c$volact))\n",
    "F1(table(as.vector(rf1_1c_pred$predict), test1c$volact))\n",
    "ACC(table(as.vector(rf6_1c_pred$predict), test6c$volact))\n",
    "F1(table(as.vector(rf6_1c_pred$predict), test6c$volact))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
